{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c55fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from utils.code_extractor import extract_python_code\n",
    "from utils.code_saver import save_code\n",
    "from utils.code_executor import PythonCodeExecutor\n",
    "from agents.code_verifier_agent import CodeVerifierAgent\n",
    "\n",
    "class EDAAgent:\n",
    "    def __init__(self, thread_id):\n",
    "\n",
    "        with open(\"configs/config.json\", \"r\") as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        # self.api_key = config[\"openai_api_key\"]\n",
    "        os.environ['OPENAI_API_KEY'] = config[\"openai_api_key\"]\n",
    "        self.model_name = config['openai_model_name']\n",
    "\n",
    "        self.thread_id = thread_id\n",
    "        self.info_json = f\"./ml_task_memory/info_{self.thread_id}.json\"\n",
    "        self.agent_output_dir = f\"./output/{self.thread_id}/\"\n",
    "        self.agent_output_filename = 'eda_agent.json'\n",
    "\n",
    "\n",
    "    def get_response(self, prompt):\n",
    "        client = OpenAI()\n",
    "        response = client.responses.create(\n",
    "            model=self.model_name,\n",
    "            input=prompt\n",
    "        )\n",
    "\n",
    "        return response\n",
    "\n",
    "    def get_planning_prompt(self):\n",
    "        with open(self.info_json, 'r') as f:\n",
    "            info_json = json.load(f)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "            You are an expert Data Scientist. Your task is to generate a step-by-step list of Exploratory Data Analysis (EDA) tasks tailored to the given data type, with a focus on supporting downstream agents for feature engineering, model training, and evaluation.\n",
    "\n",
    "            - All information about the data like dataset path, task intent, target column, etc. can be found in {self.info_json}.\n",
    "\n",
    "            Requirements:\n",
    "            - Information about the data can be acessed from {info_json}.\n",
    "            - All tasks should be designed so that their outputs (important textual or numeric summaries, statistics, or lists) are logged into a structured JSON file, not displayed.\n",
    "            - Tasks must begin with data loading and proceed through all essential EDA steps, including identifying data types, missing values, statistical summaries, cardinality, outlier detection, and any domain-specific EDA needed for modeling.\n",
    "            - Avoid tasks that only generate visualizations unless the underlying data/summary is also saved as JSON.\n",
    "            - Each task should be written as a single string, achievable via Python, and focus on producing outputs that can be consumed programmatically by downstream agents.\n",
    "            - Do not output code, explanations, or any text outside the Python list of task descriptions.\n",
    "\n",
    "            Output format:\n",
    "            A Python list of EDA task descriptions as strings, with each task specifically designed so its results are logged into a JSON file for use by downstream agents. \n",
    "            - Make sure to save all the results in the JSON file and all values to be logged are JSON serializable\n",
    "            - the output should be saved at {self.agent_output_dir} and the output json file should be named as {self.agent_output_filename}.\n",
    "            \"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def get_code_gen_prompt(self, text):\n",
    "        prompt = f\"\"\"\n",
    "            You are an expert in Data Science and Machine Learning. Your task is to write Python code that performs the following operations:\n",
    "\n",
    "            Task: \n",
    "            - Write Python code for {text}, which is a Data Science, Machine Learning, or EDA task.\n",
    "            \n",
    "            Logging and Saving Results:\n",
    "            - Ensure that all relevant results and outputs are saved in a JSON file.\n",
    "            - The data you log should be JSON serializable. This means using data types like lists, dictionaries, numbers, and strings.\n",
    "            - Ensure you include all relevant statistics, summaries, or results generated during the task in the JSON file. This includes intermediate results and any processed data.\n",
    "            \n",
    "            JSON Output Requirements:\n",
    "            - The output should be saved at {self.agent_output_dir}.\n",
    "            - The JSON file should be named {self.agent_output_filename}.\n",
    "            - Make sure the data in the JSON file is structured logically, with clear keys and values for each result.\n",
    "\n",
    "            File Handling:\n",
    "            - Ensure that the file is properly written and closed after logging the results. The output file should be created in the specified directory, and it should be accessible without errors.\n",
    "        \"\"\"\n",
    "        return prompt\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        # Plan the work\n",
    "        planning_prompt = self.get_planning_prompt()\n",
    "        plan_response = self.get_response(planning_prompt)\n",
    "        \n",
    "        # code generation\n",
    "        list_text = plan_response.output_text\n",
    "        code_gen_prompt = self.get_code_gen_prompt(list_text)\n",
    "        code_gen_response = self.get_response(code_gen_prompt)\n",
    "\n",
    "        extracted_code = extract_python_code(code_gen_response.output_text)\n",
    "\n",
    "        # Execute code and verify\n",
    "        for i in range(4):\n",
    "            print(f\"attempt: {i} ------>\")\n",
    "            executor = PythonCodeExecutor()\n",
    "            code = extracted_code[0]\n",
    "            result = executor.execute(code)\n",
    "            success = result.success\n",
    "\n",
    "            print('----------------')\n",
    "            print(result.stderr)\n",
    "            print('----------------')\n",
    "\n",
    "            if not success:\n",
    "                # verify code\n",
    "                codevef = CodeVerifierAgent(self.thread_id, list_text, code, result.stderr)\n",
    "                extracted_code = codevef.run()\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        file_path = f\"./output/{self.thread_id}/eda.py\"\n",
    "        save_code(file_path, extracted_code[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7092da6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensured directory './output/2' exists.\n",
      "Code successfully saved to './output/2/eda.py'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_train = EDAAgent(2)\n",
    "model_train.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abee0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6dea88a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from utils.code_extractor import extract_python_code\n",
    "from utils.code_saver import save_code\n",
    "from utils.code_executor import PythonCodeExecutor\n",
    "\n",
    "class FeatureEngineeringAgent:\n",
    "    def __init__(self, thread_id):\n",
    "\n",
    "        with open(\"configs/config.json\", \"r\") as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        # self.api_key = config[\"openai_api_key\"]\n",
    "        os.environ['OPENAI_API_KEY'] = config[\"openai_api_key\"]\n",
    "\n",
    "        self.thread_id = thread_id\n",
    "        self.info_json = f\"./ml_task_memory/info_{self.thread_id}.json\"\n",
    "        self.eda_json_output = f\"./output/{self.thread_id}/eda_agent.json\"\n",
    "\n",
    "\n",
    "    def get_response(self, prompt):\n",
    "        client = OpenAI()\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-4.1-nano-2025-04-14\",\n",
    "            input=prompt\n",
    "        )\n",
    "\n",
    "        return response\n",
    "\n",
    "    def get_planning_prompt(self):\n",
    "        with open(self.eda_json_output, 'r') as f:\n",
    "            eda_json_output = json.load(f)\n",
    "\n",
    "\n",
    "        with open(self.info_json, 'r') as f:\n",
    "            info_json = json.load(f)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "            You are an expert Machine Learning Engineer. Your task is to generate a step-by-step list of tasks for performing feature engineering.\n",
    "\n",
    "            Context:\n",
    "            - A previous agent (EDA agent) has already analyzed the dataset and generated a structured JSON file named {self.eda_json_output}. This file contains outputs such as data types, missing value statistics, cardinality, outlier info, and distribution summaries.\n",
    "\n",
    "            Requirements:\n",
    "            - Information about the data can be accessed from {info_json}.\n",
    "            - All tasks should use insights from the EDA JSON file ({eda_json_output}) where appropriate.\n",
    "            - task_intent from {info_json} indicates the type of machine learning task to be performed, which might influence feature engineering strategies (e.g., target encoding for classification).\n",
    "            - Each task should produce outputs (such as engineered features, transformed datasets, or feature engineering choices) that are JSON-serializable and must be logged into a structured JSON file. This JSON will be consumed by downstream agents for model training, deployment, explanation, or monitoring.\n",
    "            - Consider various feature engineering techniques including handling missing values, encoding categorical features, scaling numerical features, creating new features from existing ones (e.g., polynomial features, interaction terms, date-time features), and dimensionality reduction if necessary.\n",
    "            - Tasks must begin with reading the EDA output and continue through various feature transformation steps, culminating in a ready-to-use dataset for model training.\n",
    "            - Avoid any tasks that only generate visualizations unless their summaries or values are saved in structured form.\n",
    "            - Each task must be expressed as a single string that could be executed in Python and designed to run sequentially.\n",
    "            - Do not output code, explanations, or any text outside the Python list of task descriptions.\n",
    "\n",
    "            Input:\n",
    "            - Information about the data: JSON File\n",
    "            - EDA results: structured JSON file\n",
    "\n",
    "            Output format:\n",
    "            A Python list of feature engineering task descriptions as strings, with each task specifically designed so its results are logged into a structured JSON file for use by downstream agents. Make sure to save all the results in the JSON file and all values to be logged are JSON serializable.\n",
    "            \"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def get_code_gen_prompt(self, text):\n",
    "        prompt = f\"write python code for {text}\"\n",
    "        return prompt\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        # Plan the work\n",
    "        planning_prompt = self.get_planning_prompt()\n",
    "        plan_response = self.get_response(planning_prompt)\n",
    "        \n",
    "        # code generation\n",
    "        list_text = plan_response.output_text\n",
    "        code_gen_prompt = self.get_code_gen_prompt(list_text)\n",
    "        code_gen_response = self.get_response(code_gen_prompt)\n",
    "\n",
    "        extracted_code = extract_python_code(code_gen_response.output_text)\n",
    "\n",
    "        file_path = f\"./output/{self.thread_id}/feature_engineering.py\"\n",
    "        save_code(file_path, extracted_code[0])\n",
    "\n",
    "        executor = PythonCodeExecutor()\n",
    "        code = extracted_code[0]\n",
    "        result = executor.execute(code)\n",
    "\n",
    "        print(result.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4dd6a8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensured directory './output/2' exists.\n",
      "Code successfully saved to './output/2/feature_engineering.py'\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/var/folders/s1/s3f_rpyj6zv3xss6397vnq0m0000gn/T/tmp8wmz0a3x.py\"\u001b[0m, line \u001b[35m102\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    if \u001b[1;31mabs(corr_value) >= 0.9\u001b[0m:\n",
      "       \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/anaconda3/envs/agentml/lib/python3.13/site-packages/pandas/core/generic.py\"\u001b[0m, line \u001b[35m1577\u001b[0m, in \u001b[35m__nonzero__\u001b[0m\n",
      "    raise ValueError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "\u001b[1;35mValueError\u001b[0m: \u001b[35mThe truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_train = FeatureEngineeringAgent(2)\n",
    "model_train.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f24cf48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f51680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from utils.code_extractor import extract_python_code\n",
    "from utils.code_saver import save_code\n",
    "from utils.code_executor import PythonCodeExecutor\n",
    "from agents.code_verifier_agent import CodeVerifierAgent\n",
    "\n",
    "class ModelTrainingAgent:\n",
    "    def __init__(self, thread_id):\n",
    "\n",
    "        with open(\"configs/config.json\", \"r\") as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        # self.api_key = config[\"openai_api_key\"]\n",
    "        os.environ['OPENAI_API_KEY'] = config[\"openai_api_key\"]\n",
    "        self.model_name = config['openai_model_name']\n",
    "\n",
    "        self.thread_id = thread_id\n",
    "        self.info_json = f\"./ml_task_memory/info_{self.thread_id}.json\"\n",
    "        self.eda_json_output = f\"./output/{self.thread_id}/eda_agent.json\"\n",
    "        self.output_directory = f\"./output/{self.thread_id}/\"\n",
    "        self.output_json = f\"./output/{self.thread_id}/model_training.json\"\n",
    "\n",
    "\n",
    "    def get_response(self, prompt):\n",
    "        client = OpenAI()\n",
    "        response = client.responses.create(\n",
    "            model=self.model_name,\n",
    "            input=prompt\n",
    "        )\n",
    "\n",
    "        return response\n",
    "\n",
    "    def get_planning_prompt(self):\n",
    "        with open(self.eda_json_output, 'r') as f:\n",
    "            eda_json_output = json.load(f)\n",
    "\n",
    "\n",
    "        with open(self.info_json, 'r') as f:\n",
    "            info_json = json.load(f)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "            You are an expert Machine Learning Engineer. Your task is to generate a step-by-step list of tasks for training and evaluating a machine learning model using structured tabular data.\n",
    "\n",
    "            Context:\n",
    "            - A previous agent (EDA agent) has already analyzed the dataset and generated a structured JSON file named {self.eda_json_output}. This file contains outputs such as data types, missing value statistics, cardinality, outlier info, and distribution summaries.\n",
    "\n",
    "            Requirements:\n",
    "            - Information about the data can be acessed from {info_json}.\n",
    "            - All tasks should use insights from the EDA JSON file ({eda_json_output}) where appropriate.\n",
    "            - task_intent from {info_json} indicates the type of machine learning task to be performed.\n",
    "            - Each task should produce outputs (such as selected features, cleaned dataset paths, model hyperparameters, evaluation metrics, or model file paths) that are JSON-serializable and must be logged into a structured JSON file. This JSON will be consumed by downstream agents for deployment, explanation, or monitoring.\n",
    "            - Multiple models should be trained and evaluated. Choose wisely.\n",
    "            - Tasks must begin with reading the EDA output and continue through preprocessing, feature selection, train-test splitting, model training, evaluation, and saving of final artifacts.\n",
    "            - Avoid any tasks that only generate visualizations unless their summaries or values are saved in structured form.\n",
    "            - Each task must be expressed as a single string that could be executed in Python and designed to run sequentially.\n",
    "            - Do not output code, explanations, or any text outside the Python list of task descriptions.\n",
    "\n",
    "            Input:\n",
    "            - Information about the data: JSON FIle\n",
    "            - EDA results: structured JSON file \n",
    "\n",
    "            Output format:\n",
    "            A Python list of model training task descriptions as strings, with each task specifically designed so its results are logged into a structured JSON file for use by downstream agents. Make sure to save all the results in the JSON file and all values to be logged are JSON serializable.\n",
    "            - make sure to save all files at {self.output_directory} \n",
    "            - the name for output log should be saved with the name {self.output_json}\n",
    "            \"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def get_code_gen_prompt(self, text):\n",
    "        prompt = f\"\"\"\n",
    "\n",
    "            You are an expert in Data Science and Machine Learning. Your task is to write Python code that performs the following operations:\n",
    "\n",
    "            Task: \n",
    "            - Write Python code for {text}, which is a Data Science, Machine Learning, or EDA task.\n",
    "            \n",
    "            Logging and Saving Results:\n",
    "            - Ensure that all relevant results and outputs are saved in a JSON file.\n",
    "            - The data you log should be JSON serializable. This means using data types like lists, dictionaries, numbers, and strings.\n",
    "            - The JSON file should contain all the logs and relevant results from the task.\n",
    "\n",
    "            JSON Output Requirements:\n",
    "            - The output should be saved at {self.output_directory}.\n",
    "            - The JSON file should be named {self.output_json}.\n",
    "            - Make sure the data in the JSON file is structured logically, with clear keys and values for each result.\n",
    "\n",
    "            File Handling:\n",
    "            - Ensure that the file is properly written and closed after logging the results. The output file should be created in the specified directory, and it should be accessible without errors.\n",
    "        \"\"\"\n",
    "        return prompt\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        # Plan the work\n",
    "        planning_prompt = self.get_planning_prompt()\n",
    "        plan_response = self.get_response(planning_prompt)\n",
    "        \n",
    "        # code generation\n",
    "        list_text = plan_response.output_text\n",
    "        code_gen_prompt = self.get_code_gen_prompt(list_text)\n",
    "        code_gen_response = self.get_response(code_gen_prompt)\n",
    "\n",
    "        extracted_code = extract_python_code(code_gen_response.output_text)\n",
    "        \n",
    "        # Execute code and verify\n",
    "        for i in range(4):\n",
    "            print(f\"attempt: {i} ------>\")\n",
    "            executor = PythonCodeExecutor()\n",
    "            code = extracted_code[0]\n",
    "            result = executor.execute(code)\n",
    "            success = result.success\n",
    "\n",
    "            print('----------------')\n",
    "            print(result.stderr)\n",
    "            print('----------------')\n",
    "\n",
    "            if not success:\n",
    "                # verify code\n",
    "                codevef = CodeVerifierAgent(self.thread_id, list_text, code, result.stderr)\n",
    "                extracted_code = codevef.run()\n",
    "            else:\n",
    "                break\n",
    "\n",
    "                \n",
    "\n",
    "        file_path = f\"./output/{self.thread_id}/model_training.py\"\n",
    "        save_code(file_path, extracted_code[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f436ebb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempt: 0 ------>\n",
      "----------------\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/var/folders/s1/s3f_rpyj6zv3xss6397vnq0m0000gn/T/tmpf01uevv3.py\"\u001b[0m, line \u001b[35m76\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    feat: \u001b[31meda\u001b[0m\u001b[1;31m[\"feature_summary\"]\u001b[0m[feat].get(\"missing_count\", None)\n",
      "          \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[1;35mKeyError\u001b[0m: \u001b[35m'feature_summary'\u001b[0m\n",
      "\n",
      "----------------\n",
      "attempt: 1 ------>\n",
      "----------------\n",
      "\n",
      "----------------\n",
      "attempt: 2 ------>\n",
      "----------------\n",
      "\n",
      "----------------\n",
      "attempt: 3 ------>\n",
      "----------------\n",
      "\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "model_train = ModelTrainingAgent(3)\n",
    "extracted = model_train.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62b5bb62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['import json\\nimport os\\nimport sys\\nimport traceback\\nimport pandas as pd\\nimport numpy as np\\n\\n# Dependency checks\\nmissing_deps = []\\nfor dep, pkg in [\\n    (\\'xgboost\\', \\'xgboost\\'), (\\'lightgbm\\', \\'lightgbm\\'),\\n    (\\'sklearn\\', \\'scikit-learn\\'), (\\'joblib\\', \\'joblib\\')]:\\n    try:\\n        __import__(dep)\\n    except ImportError:\\n        missing_deps.append(pkg)\\nif missing_deps:\\n    print(\"[FATAL] Missing required packages:\", \", \".join(missing_deps))\\n    sys.exit(1)\\n\\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold\\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom xgboost import XGBRegressor\\nfrom lightgbm import LGBMRegressor\\nfrom joblib import dump\\n\\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\\nfrom itertools import product\\n\\n# ========== CONFIG/CONSTANTS ==========\\nOUTPUT_DIR = \"./output/3\"\\nEDA_JSON = os.path.join(OUTPUT_DIR, \"eda_agent.json\")\\nMT_JSON = os.path.join(OUTPUT_DIR, \"model_training.json\")\\nos.makedirs(OUTPUT_DIR, exist_ok=True)\\nSEED = 42\\n\\n# ========== LOGGING HELPERS ==========\\nlogs = {}\\n\\ndef log(key, value):\\n    \"\"\"Helper for top-level logging\"\"\"\\n    logs[key] = value\\n\\ndef nested_log(path, value):\\n    \"\"\"Support nested dict logging using \\'a.b.c\\' as path\"\"\"\\n    d = logs\\n    keys = path.split(\\'.\\')\\n    for k in keys[:-1]:\\n        d = d.setdefault(k, {})\\n    d[keys[-1]] = value\\n\\ndef save_logs():\\n    try:\\n        with open(MT_JSON, \"w\") as f:\\n            json.dump(logs, f, indent=2)\\n    except Exception as e:\\n        print(f\"CRITICAL: Could not write logs - {e}\")\\n\\ndef log_and_exit(msg, exception=None):\\n    \"\"\"Log error, save logs, print traceback and exit\"\"\"\\n    log(\"error\", msg)\\n    if exception is not None:\\n        log(\"exception_trace\", traceback.format_exc())\\n    save_logs()\\n    print(f\"[ERROR]: {msg}\")\\n    if exception:\\n        print(traceback.format_exc())\\n    sys.exit(1)\\n\\ndef assert_not_empty(obj, msg):\\n    if obj is None or (hasattr(obj, \\'__len__\\') and len(obj) == 0):\\n        log_and_exit(msg)\\n\\n# ========== SAFE MODEL MAP ==========\\nMODEL_MAP = {\\n    \\'LinearRegression\\': LinearRegression,\\n    \\'RandomForestRegressor\\': RandomForestRegressor,\\n    \\'XGBRegressor\\': XGBRegressor,\\n    \\'LGBMRegressor\\': LGBMRegressor,\\n}\\n\\n# ========== 1. LOAD EDA JSON ==========\\nif not os.path.isfile(EDA_JSON):\\n    log_and_exit(f\"EDA JSON file \\'{EDA_JSON}\\' not found.\")\\n\\ntry:\\n    with open(EDA_JSON, \"r\") as f:\\n        eda = json.load(f)\\nexcept Exception as e:\\n    log_and_exit(\"Failed reading/parsing EDA JSON.\", e)\\n\\n# Defensive: check for required top-level keys in EDA JSON\\nREQUIRED_EDA_KEYS = [\"feature_summary\", \"task\", \"target_column\", \"data_path\"]\\nfor key in REQUIRED_EDA_KEYS:\\n    if key not in eda:\\n        log_and_exit(f\"Required key \\'{key}\\' missing from EDA JSON.\")\\n\\nfeature_summary = eda[\"feature_summary\"]\\nif not isinstance(feature_summary, dict) or len(feature_summary) == 0:\\n    log_and_exit(\"\\'feature_summary\\' missing or empty in EDA JSON. Cannot proceed with model training.\")\\n\\ntask_intent = eda.get(\"task\", \"\")\\ntarget_col = eda.get(\"target_column\", \"Yield\")\\n\\ninput_features = [f for f in feature_summary.keys() if f != target_col]\\nfeature_info = {\\n    feat: {\\n        \"warnings\": feature_summary.get(feat, {}).get(\"warnings\", []) or [],\\n        \"recommendations\": feature_summary.get(feat, {}).get(\"recommendations\", []) or []\\n    } for feat in input_features\\n}\\nlog(\\'eda\\', {\\n    \"task\": task_intent,\\n    \"target\": target_col,\\n    \"features\": input_features,\\n    \"feature_info\": feature_info,\\n    \"initial_info\": eda.get(\"data_info\", {}),\\n    \"eda_insights\": eda.get(\"eda_insights\", {})\\n})\\n\\n# ========== 2. MISSING VALUE CHECK ==========\\nmissing_info = {\\n    feat: feature_summary.get(feat, {}).get(\"missing_count\", None)\\n    for feat in (input_features + [target_col])\\n}\\nall_missing_zero = all(missing_info[f] == 0 for f in missing_info if missing_info[f] is not None)\\nnested_log(\\'missing_value_check.result\\', all_missing_zero)\\nnested_log(\\'missing_value_check.missing_info\\', missing_info)\\n\\n# ========== 3. OUTLIER ANALYSIS ==========\\noutlier_dict = {}\\noutlier_indices = set()\\nfor feat in input_features + [target_col]:\\n    fs = feature_summary.get(feat, {})\\n    outliers = fs.get(\"outlier_indices\", [])\\n    # Defensive: coerce to list of int\\n    out_indices = []\\n    for idx in outliers:\\n        try:\\n            int_idx = int(idx)\\n            if int_idx >= 0:  # pandas index cannot be negative\\n                out_indices.append(int_idx)\\n        except Exception:\\n            continue\\n    outlier_dict[feat] = {\"count\": len(out_indices), \"indices\": out_indices}\\n    if feat == target_col and out_indices:\\n        outlier_indices.update(out_indices)\\n\\nnested_log(\\'outlier_analysis\\', outlier_dict)\\nnested_log(\\'outlier_analysis.target_outlier_count\\', len(outlier_indices))\\n\\n# ========== LOAD DATA ==========\\norig_data_path = eda.get(\"data_path\", \"\")\\nif not orig_data_path or not os.path.isfile(orig_data_path):\\n    log_and_exit(f\"Original data file not found: {orig_data_path}\")\\ntry:\\n    df = pd.read_csv(orig_data_path)\\nexcept Exception as e:\\n    log_and_exit(f\"Failed to load data file: {orig_data_path}\", e)\\n\\n# Remove outlier rows in target column (by index)\\nif outlier_indices:\\n    outlier_indices_filtered = [i for i in outlier_indices if 0 <= i < len(df)]\\n    df_no_target_outliers = df.drop(index=outlier_indices_filtered, errors=\\'ignore\\').reset_index(drop=True)\\nelse:\\n    df_no_target_outliers = df.copy().reset_index(drop=True)\\n\\nno_outlier_csv = os.path.join(OUTPUT_DIR, \"train_data_no_target_outliers.csv\")\\ntry:\\n    df_no_target_outliers.to_csv(no_outlier_csv, index=False)\\nexcept Exception as e:\\n    log_and_exit(\"Could not save outlier-removed data.\", e)\\nnested_log(\\'outlier_analysis.no_target_outlier_data_path\\', no_outlier_csv)\\n\\n# ========== 4. SKEWNESS/KURTOSIS ==========\\nnumerical_feats = [\\n    feat for feat in input_features\\n    if feature_summary.get(feat, {}).get(\"type\", \"numerical\") == \"numerical\"\\n]\\n\\nskew_kurt_dict = {}\\npreprocess_actions = {}\\nfor feat in numerical_feats + [target_col]:\\n    fs = feature_summary.get(feat, {})\\n    skew = fs.get(\"skewness\", None)\\n    kurt = fs.get(\"kurtosis\", None)\\n    recs = []\\n    action = None\\n    if skew is not None and (abs(skew) > 1 or (kurt is not None and kurt > 5)):\\n        recs.append(\"High skewness or kurtosis\")\\n        if feat == target_col:\\n            action = \"Yeo-Johnson\"\\n        elif any(sub in feat for sub in [\"Rainfall\", \"Phosphorus\", \"Potassium\"]):\\n            action = \"log(x+1)\"\\n        else:\\n            action = \"None\"\\n    else:\\n        action = \"None\"\\n    skew_kurt_dict[feat] = {\\n        \"skewness\": skew,\\n        \"kurtosis\": kurt,\\n        \"remarks\": recs\\n    }\\n    preprocess_actions[feat] = action\\n\\nnested_log(\"skewness_kurtosis\", skew_kurt_dict)\\nnested_log(\"preprocessing.feature_transformations\", preprocess_actions)\\n\\n# ========== 5. ONE-HOT ENCODE CATEGORICAL ==========\\ncat_feat = \\'Crop\\'\\nif cat_feat not in df_no_target_outliers.columns:\\n    available_cols = list(df_no_target_outliers.columns)\\n    log_and_exit(f\"Categorical feature \\'{cat_feat}\\' not found in data columns: {available_cols}\")\\n\\ncat_mapping = sorted([str(v) for v in df_no_target_outliers[cat_feat].dropna().unique()])\\nif len(cat_mapping) == 0:\\n    log_and_exit(\"No unique crops found in categorical feature \\'Crop\\'.\")\\n\\ntry:\\n    encoder = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\\n    encoded_crop = encoder.fit_transform(df_no_target_outliers[[cat_feat]])\\n    if hasattr(encoder, \"get_feature_names_out\"):\\n        encoded_crop_colnames = encoder.get_feature_names_out([cat_feat]).tolist()\\n    else:\\n        encoded_crop_colnames = [f\"{cat_feat}_{cat}\" for cat in encoder.categories_[0]]\\n    # Defensive: mapping as string keys\\n    cat_mapping_dict = {str(cat): int(i) for i, cat in enumerate(encoder.categories_[0])}\\n    nested_log(\\'encoding.Crop.mapping\\', cat_mapping_dict)\\nexcept Exception as e:\\n    log_and_exit(\"Error in one-hot encoding of \\'Crop\\'.\", e)\\n\\n# ========== 5B. CREATE X, y AND TRANSFORM FEATURES ==========\\nX = df_no_target_outliers[input_features].copy()\\ny = df_no_target_outliers[target_col].copy()\\n\\n# log(x+1) transformation for skewed input features (if any)\\nfor feat in input_features:\\n    if preprocess_actions.get(feat) == \"log(x+1)\":\\n        try:\\n            X[feat] = np.log1p(X[feat].astype(float))\\n        except Exception as e:\\n            log_and_exit(f\"log(x+1) failed for {feat}.\", e)\\n\\n# Replace categorical with one-hot encoded columns\\nif cat_feat in X.columns:\\n    X = X.drop(columns=[cat_feat])\\nfor i, col in enumerate(encoded_crop_colnames):\\n    X[col] = encoded_crop[:, i]\\n\\nprocessed_features = list(X.columns)  # After all encoding\\n\\n# Only scale numeric columns (not OHE binary columns)\\nnum_feats = [col for col in processed_features if\\n             (X[col].dtype.kind in \\'fi\\' and col not in encoded_crop_colnames)]\\nif num_feats:\\n    try:\\n        X[num_feats] = X[num_feats].astype(float)\\n        scaler = StandardScaler()\\n        X[num_feats] = scaler.fit_transform(X[num_feats])\\n    except Exception as e:\\n        log_and_exit(f\"StandardScaler failed on columns {num_feats}.\", e)\\n\\n# ========== 5C. SAVE PREPROCESSED DATA ==========\\nprocessed_csv = os.path.join(OUTPUT_DIR, \"train_data_preprocessed.csv\")\\nprocessed_data = X.copy()\\nprocessed_data[target_col] = y.values  # Defensive in case index mismatch\\ntry:\\n    processed_data.to_csv(processed_csv, index=False)\\nexcept Exception as e:\\n    log_and_exit(\"Could not save preprocessed data.\", e)\\nnested_log(\\'preprocessing.processed_data_path\\', processed_csv)\\nnested_log(\\'preprocessing.processed_features\\', processed_features)\\nnested_log(\\'encoding.Crop.encoded_columns\\', encoded_crop_colnames)\\n\\n# ========== 6. FEATURE SELECTION ==========\\nleakage_feats = [\\n    feat for feat in input_features\\n    if any(\"leakage\" in str(warning).lower() for warning in feature_info.get(feat, {}).get(\\'warnings\\', []))\\n]\\ncorrelations = eda.get(\\'correlation_matrix\\', {})\\ndomain_selected = eda.get(\\'eda_insights\\', {}).get(\\'recommended_features\\') or processed_features\\n\\n# Only use features present after preprocessing, and exclude leakages\\nselected_features = [\\n    feat for feat in processed_features if feat not in leakage_feats\\n]\\n\\n# Defensive: only keep features actually in processed_data\\nselected_features = [f for f in selected_features if f in processed_data.columns and f != target_col]\\nassert_not_empty(selected_features, \"No features selected after removing potential leakage features. Check feature processing/column names.\")\\n\\nnested_log(\\'feature_selection.correlation_matrix\\', correlations)\\nnested_log(\\'feature_selection.selected_features\\', selected_features)\\nnested_log(\\'feature_selection.leakage_flags\\', leakage_feats)\\nnested_log(\\'feature_selection.domain_recommendations\\', domain_selected)\\n\\n# ========== 7. TRAIN/TEST SPLIT ==========\\ntry:\\n    strat_col_for_split = None\\n    if len(cat_mapping) > 1 and (cat_feat in df_no_target_outliers.columns):\\n        strat_col_for_split = df_no_target_outliers[cat_feat]\\n    else:\\n        strat_col_for_split = None\\n\\n    split_kwargs = dict(test_size=0.2, random_state=SEED, shuffle=True)\\n    split_stratify = strat_col_for_split if (\\n        strat_col_for_split is not None and\\n        df_no_target_outliers[cat_feat].nunique() > 1\\n    ) else None\\n    nrows = len(df_no_target_outliers)\\n    all_indices = np.arange(nrows)\\n    if split_stratify is not None:\\n        train_idx, test_idx = train_test_split(\\n            all_indices,\\n            stratify=strat_col_for_split,\\n            **split_kwargs\\n        )\\n    else:\\n        train_idx, test_idx = train_test_split(\\n            all_indices,\\n            **split_kwargs\\n        )\\n    train_df = processed_data.iloc[train_idx].reset_index(drop=True)\\n    test_df = processed_data.iloc[test_idx].reset_index(drop=True)\\n    train_csv = os.path.join(OUTPUT_DIR, \"train_set.csv\")\\n    test_csv = os.path.join(OUTPUT_DIR, \"test_set.csv\")\\n    train_df.to_csv(train_csv, index=False)\\n    test_df.to_csv(test_csv, index=False)\\n    split_stats = {\\n        \"train_shape\": train_df.shape,\\n        \"test_shape\": test_df.shape,\\n        \"split_train_indices\": [int(i) for i in train_idx],\\n        \"split_test_indices\": [int(i) for i in test_idx],\\n        \"random_seed\": SEED\\n    }\\n    # Defensive: check\\n    if train_df.empty or test_df.empty:\\n        log_and_exit(\"Train or test split resulted in empty dataframes.\")\\nexcept Exception as e:\\n    log_and_exit(\"Failed during train/test split.\", e)\\n\\nnested_log(\\'split.train_path\\', train_csv)\\nnested_log(\\'split.test_path\\', test_csv)\\nnested_log(\\'split.stats\\', split_stats)\\n\\n# ========== 8. MODEL SETUPS ==========\\nmodel_setups = {\\n    \"LinearRegression\": {\\n        \"class\": \"LinearRegression\",\\n        \"hyperparameters\": {}\\n    },\\n    \"RandomForestRegressor\": {\\n        \"class\": \"RandomForestRegressor\",\\n        \"hyperparameters\": {\\n            \"n_estimators\": [100, 200],\\n            \"max_depth\": [None, 8, 12],\\n            \"random_state\": [SEED]\\n        }\\n    },\\n    \"XGBRegressor\": {\\n        \"class\": \"XGBRegressor\",\\n        \"hyperparameters\": {\\n            \"n_estimators\": [100, 200],\\n            \"max_depth\": [3, 6, 10],\\n            \"learning_rate\": [0.05, 0.1],\\n            \"random_state\": [SEED]\\n        }\\n    },\\n    \"LGBMRegressor\": {\\n        \"class\": \"LGBMRegressor\",\\n        \"hyperparameters\": {\\n            \"n_estimators\": [100, 200],\\n            # LightGBM\\'s max_depth: None disables max_depth limit, -1 disables limit in LGBM native\\n            \"max_depth\": [None, 8, 12],\\n            \"learning_rate\": [0.05, 0.1],\\n            \"random_state\": [SEED]\\n        }\\n    }\\n}\\nnested_log(\\'model_setups\\', model_setups)\\n\\n# ========== 9. MODEL TRAINING & CV ==========\\ncv_results = {}\\nouter_X = train_df[selected_features]\\nouter_y = train_df[target_col]\\nouter_folds = min(5, len(train_df)) if len(train_df) > 1 else 2\\n\\n# For stratified CV, we could use crop assignment if >1 class\\nstrat_groups_cv = None\\nif len(cat_mapping) > 1 and (cat_feat in df_no_target_outliers.columns):\\n    orig_indices_in_train = np.array(train_idx)\\n    strat_groups_cv = df_no_target_outliers.iloc[orig_indices_in_train][cat_feat].reset_index(drop=True)\\ncrossval = (\\n    StratifiedKFold(n_splits=outer_folds, shuffle=True, random_state=SEED)\\n    if strat_groups_cv is not None and len(set(strat_groups_cv)) > 1\\n    else KFold(n_splits=outer_folds, shuffle=True, random_state=SEED)\\n)\\n\\nfor model_name, model_setup in model_setups.items():\\n    model_class = MODEL_MAP[model_name]\\n    param_grid = model_setup[\"hyperparameters\"]\\n    # Special case: support LightGBM\\'s max_depth=-1\\n    if model_name == \"LGBMRegressor\":\\n        mg = param_grid.get(\"max_depth\", [])\\n        fixed = []\\n        for v in mg:\\n            if v == -1:\\n                fixed.append(None)\\n            else:\\n                fixed.append(v)\\n        param_grid = dict(param_grid)\\n        param_grid[\"max_depth\"] = fixed\\n    param_names = list(param_grid)\\n    param_combos = [dict(zip(param_names, x))\\n                    for x in product(*[param_grid[k] for k in param_names])] if param_grid else [{}]\\n    if model_name == \"LinearRegression\":\\n        param_combos = [{}]\\n    model_cv_list = []\\n    for params in param_combos:\\n        try:\\n            model = model_class(**params) if params else model_class()\\n            scores = {\"folds\": [], \"mae\": [], \"rmse\": [], \"r2\": []}\\n            skf = crossval\\n            strat_groups = strat_groups_cv if isinstance(skf, StratifiedKFold) else None\\n            fold_split = skf.split(outer_X, strat_groups) if strat_groups is not None else skf.split(outer_X)\\n            for train_idx_cv, valid_idx_cv in fold_split:\\n                X_train_cv, X_valid_cv = outer_X.iloc[train_idx_cv], outer_X.iloc[valid_idx_cv]\\n                y_train_cv, y_valid_cv = outer_y.iloc[train_idx_cv], outer_y.iloc[valid_idx_cv]\\n                # Defensive: check nonempty folds\\n                if X_train_cv.empty or X_valid_cv.empty:\\n                    continue\\n                # Transformation for target (Yeo-Johnson) if specified\\n                if preprocess_actions.get(target_col) == \"Yeo-Johnson\":\\n                    pt_cv = PowerTransformer(method=\"yeo-johnson\")\\n                    y_train_fit = pt_cv.fit_transform(y_train_cv.values.reshape(-1, 1)).flatten()\\n                    model.fit(X_train_cv, y_train_fit)\\n                    preds = model.predict(X_valid_cv)\\n                    preds_inversed = pt_cv.inverse_transform(preds.reshape(-1, 1)).flatten()\\n                    val_y_to_compare = y_valid_cv\\n                else:\\n                    model.fit(X_train_cv, y_train_cv)\\n                    preds_inversed = model.predict(X_valid_cv)\\n                    val_y_to_compare = y_valid_cv\\n                scores[\"folds\"].append([int(i) for i in valid_idx_cv])\\n                scores[\"mae\"].append(float(mean_absolute_error(val_y_to_compare, preds_inversed)))\\n                scores[\"rmse\"].append(float(np.sqrt(mean_squared_error(val_y_to_compare, preds_inversed))))\\n                scores[\"r2\"].append(float(r2_score(val_y_to_compare, preds_inversed)))\\n            model_cv_list.append({\\n                \"params\": params,\\n                \"cv_scores\": scores\\n            })\\n        except Exception as e:\\n            err_msg = f\"{params}: {str(e)}\"\\n            log(f\"cv_train_error_{model_name}\", err_msg)\\n            save_logs()\\n    cv_results[model_name] = model_cv_list\\n\\nnested_log(\\'cv_results\\', cv_results)\\n\\n# ========== 10. TEST EVALUATION ==========\\neval_results = {}\\ntest_X = test_df[selected_features]\\ntest_y = test_df[target_col]\\nfeature_importances = {}\\n\\nfor model_name, model_setup in model_setups.items():\\n    # Pick best params (lowest mean RMSE), but skip models with no results\\n    result_list = cv_results.get(model_name, [])\\n    if not result_list:\\n        eval_results[model_name] = {\"error\": \"No successful CV runs\", \"mae\": None, \"rmse\": None, \"r2\": None, \"hyperparameters\": {}}\\n        feature_importances[model_name] = {}\\n        continue\\n    sorted_list = sorted(\\n        result_list,\\n        key=lambda x: np.mean(x[\\'cv_scores\\'][\\'rmse\\']) if x[\\'cv_scores\\'][\\'rmse\\'] else np.inf\\n    )\\n    best_params = sorted_list[0][\"params\"] if sorted_list else {}\\n    model_class = MODEL_MAP[model_name]\\n    try:\\n        model = model_class(**best_params) if best_params else model_class()\\n        # Fit on full train data\\n        if preprocess_actions.get(target_col) == \"Yeo-Johnson\":\\n            pt = PowerTransformer(method=\"yeo-johnson\")\\n            y_train_fit = pt.fit_transform(outer_y.values.reshape(-1, 1)).flatten()\\n            model.fit(outer_X, y_train_fit)\\n            preds_test = model.predict(test_X)\\n            preds_test_inversed = pt.inverse_transform(preds_test.reshape(-1, 1)).flatten()\\n        else:\\n            model.fit(outer_X, outer_y)\\n            preds_test_inversed = model.predict(test_X)\\n        valid_preds = preds_test_inversed\\n        # Defensive: check valid metrics\\n        if len(valid_preds) != len(test_y):\\n            raise ValueError(f\"Prediction shape {valid_preds.shape} does not match target {test_y.shape}\")\\n        mae = float(mean_absolute_error(test_y, valid_preds))\\n        rmse = float(np.sqrt(mean_squared_error(test_y, valid_preds)))\\n        r2val = float(r2_score(test_y, valid_preds))\\n        eval_results[model_name] = {\\n            \"mae\": mae, \"rmse\": rmse, \"r2\": r2val, \"hyperparameters\": best_params\\n        }\\n        # Importances\\n        if hasattr(model, \"feature_importances_\"):\\n            feature_importances[model_name] = {\\n                \"importances\": {\\n                    feat: float(val) for feat, val in zip(selected_features, model.feature_importances_)\\n                }\\n            }\\n        elif hasattr(model, \"coef_\"):\\n            feature_importances[model_name] = {\\n                \"coefficients\": {\\n                    feat: float(val) for feat, val in zip(selected_features, model.coef_)\\n                }\\n            }\\n        else:\\n            feature_importances[model_name] = {}\\n    except Exception as e:\\n        eval_results[model_name] = {\"error\": str(e), \"mae\": None, \"rmse\": None, \"r2\": None, \"hyperparameters\": best_params}\\n        feature_importances[model_name] = {}\\n\\nnested_log(\\'test_evaluation.results\\', eval_results)\\nnested_log(\\'test_evaluation.feature_importances\\', feature_importances)\\n\\n# ========== 11. MODEL SELECTION ==========\\nmodel_metric_table = [\\n    (m, eval_results[m][\"rmse\"],\\n     np.mean([np.mean(r[\"cv_scores\"][\"rmse\"])\\n             for r in cv_results[m]] if cv_results[m] else [np.inf]))\\n    for m in eval_results if eval_results[m].get(\"rmse\") is not None\\n]\\nif len(model_metric_table) == 0:\\n    log_and_exit(\"No successfully evaluated model found after test evaluation.\")\\nbest_model_name = sorted(model_metric_table, key=lambda x: (x[1], x[2]))[0][0]\\n\\nselection_log = {\\n    \"model_scores\": {\\n        m: {\"test_rmse\": test_rmse, \"cv_rmse\": cv_rmse}\\n        for m, test_rmse, cv_rmse in model_metric_table\\n    },\\n    \"selected_model\": best_model_name,\\n    \"rationale\": \"Lowest test RMSE (with CV RMSE as tie-breaker)\"\\n}\\nnested_log(\\'model_selection\\', selection_log)\\n\\n# ========== 12. RETRAIN FINAL MODEL & SAVE ==========\\nX_full = processed_data[selected_features]\\ny_full = processed_data[target_col]\\n\\ntry:\\n    model_class = MODEL_MAP[best_model_name]\\n    best_params = cv_results[best_model_name][0][\"params\"] if cv_results[best_model_name] else {}\\n    model = model_class(**best_params) if best_params else model_class()\\n    # Target transformation for Yeo-Johnson\\n    if preprocess_actions.get(target_col) == \"Yeo-Johnson\":\\n        pt = PowerTransformer(method=\"yeo-johnson\")\\n        y_fit_full = pt.fit_transform(y_full.values.reshape(-1, 1)).flatten()\\n        model.fit(X_full, y_fit_full)\\n        final_pipeline = Pipeline([(\"y_transform\", pt), (\"model\", model)])\\n    else:\\n        model.fit(X_full, y_full)\\n        final_pipeline = Pipeline([(\"model\", model)])\\n\\n    model_path = os.path.join(OUTPUT_DIR, \"best_model.pkl\")\\n    dump(final_pipeline, model_path)\\nexcept Exception as e:\\n    log_and_exit(\"Error during final model retraining or saving.\", e)\\n\\nnested_log(\\'model_artifacts.best_model_path\\', model_path)\\nnested_log(\\'model_artifacts.pipeline_steps\\', str(final_pipeline))\\n\\n# ========== 13. SUMMARY FOR DOWNSTREAM ==========\\nnested_log(\\'summary.final_preprocessing\\', preprocess_actions)\\nnested_log(\\'summary.selected_features\\', selected_features)\\nnested_log(\\'summary.test_train_split\\', split_stats)\\nnested_log(\\'summary.metrics\\', eval_results.get(best_model_name, {}))\\nnested_log(\\'summary.model_artifact_path\\', model_path)\\n\\n# SAVE LOGS\\nsave_logs()\\n\\nprint(f\"All steps completed. Logs and artifacts are saved in {OUTPUT_DIR}\")']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40258a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensured directory './output/thread_id' exists.\n",
      "Code successfully saved to './output/thread_id/model_training_gpt.py'\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./output/{thread_id}/model_training_gpt.py\"\n",
    "save_code(file_path, 'import os')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19509717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.code_saver' from '/Users/prajwalchaudhary/Desktop/Uni/COMP8420/AutoAgentML/utils/code_saver.py'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import utils.code_saver\n",
    "importlib.reload(utils.code_saver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "141fcbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ExecutionResult:\n",
    "    success: bool\n",
    "    stdout: str\n",
    "    stderr: str\n",
    "\n",
    "class PythonCodeExecutor:\n",
    "    def __init__(self, timeout: int = 10):\n",
    "        self.timeout = timeout\n",
    "\n",
    "    def execute(self, code: str) -> ExecutionResult:\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as temp_file:\n",
    "            temp_file.write(code)\n",
    "            temp_file_path = temp_file.name\n",
    "\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                ['python', temp_file_path],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            return ExecutionResult(\n",
    "                success=result.returncode == 0,\n",
    "                stdout=result.stdout,\n",
    "                stderr=result.stderr\n",
    "            )\n",
    "        except subprocess.TimeoutExpired as e:\n",
    "            return ExecutionResult(success=False, stdout='', stderr='Execution timed out.')\n",
    "        except Exception as e:\n",
    "            return ExecutionResult(success=False, stdout='', stderr=str(e))\n",
    "        finally:\n",
    "            os.remove(temp_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19a37364",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec = PythonCodeExecutor()\n",
    "result = exec.execute(\"import ost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5388456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Traceback (most recent call last):\\n  File \\x1b[35m\"/var/folders/s1/s3f_rpyj6zv3xss6397vnq0m0000gn/T/tmpbpi81xxp.py\"\\x1b[0m, line \\x1b[35m1\\x1b[0m, in \\x1b[35m<module>\\x1b[0m\\n    import ost\\n\\x1b[1;35mModuleNotFoundError\\x1b[0m: \\x1b[35mNo module named \\'ost\\'\\x1b[0m\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ae56e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from utils.code_extractor import extract_python_code\n",
    "from utils.code_saver import save_code\n",
    "from utils.code_executor import PythonCodeExecutor\n",
    "\n",
    "class CodeVerifierAgent:\n",
    "    def __init__(self, thread_id, task_description, code, exec_error):\n",
    "\n",
    "        with open(\"configs/config.json\", \"r\") as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        # self.api_key = config[\"openai_api_key\"]\n",
    "        os.environ['OPENAI_API_KEY'] = config[\"openai_api_key\"]\n",
    "\n",
    "        self.thread_id = thread_id\n",
    "        # self.info_json = f\"./ml_task_memory/info_{self.thread_id}.json\"\n",
    "        # self.eda_json_output = f\"./output/{self.thread_id}/eda_agent.json\"\n",
    "\n",
    "        self.task_description = task_description\n",
    "        self.code = code\n",
    "        self.exec_error = exec_error\n",
    "\n",
    "\n",
    "    def get_response(self, prompt):\n",
    "        client = OpenAI()\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-4.1-nano-2025-04-14\",\n",
    "            input=prompt\n",
    "        )\n",
    "\n",
    "        return response\n",
    "\n",
    "    def get_planning_prompt(self):\n",
    "        prompt = f\"\"\"\n",
    "            You are an expert in debugging and code correction. Your task is to generate a comprehensive plan to handle execution errors in the provided code. The code has issues that prevent it from executing correctly. You are given the following information:\n",
    "\n",
    "            Task Description:\n",
    "            {self.task_description}\n",
    "            \n",
    "            Code:\n",
    "            {self.code}\n",
    "            Execution Error:\n",
    "            {self.exec_error}\n",
    "\n",
    "            Your job is to generate a clear and actionable plan to resolve the issues in the provided code. The plan should include the following:\n",
    "\n",
    "            - Analyze the error: Review the execution error and determine which parts of the code are causing the issue.\n",
    "            - Identify the root cause: Identify whether the error is due to logical mistakes, syntax issues, missing dependencies, or other causes.\n",
    "            - Suggested steps to correct the issue: Provide a step-by-step plan for fixing the issue in the code. This can include:\n",
    "            - Fixing syntax errors or handling exceptions\n",
    "            - Adjusting logic or refactoring code\n",
    "            - Adding missing imports or dependencies\n",
    "            - Correcting variable scope or data type issues\n",
    "            - Updating method calls or object handling\n",
    "            - Verification: Include how the corrected code should be verified (e.g., through unit tests, debugging, or re-running the code).\n",
    "            - Final suggestions: Provide any additional tips to prevent similar errors in the future.\n",
    "            \n",
    "            Output format:\n",
    "            - Return a python list of steps to correct the code and address the execution error.\n",
    "            \"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def get_code_gen_prompt(self, plan):\n",
    "        prompt = f\"\"\"\"\n",
    "        You are an expert in Python programming and debugging. Based on the detailed debugging plan you received, your task is to generate the corrected version of the provided code. The plan includes steps for resolving the execution error, fixing syntax issues, and improving the code structure. Please follow these guidelines:\n",
    "\n",
    "            Task Description:\n",
    "            {self.task_description}\n",
    "            \n",
    "            Code:\n",
    "            {self.code}\n",
    "            Execution Error:\n",
    "            {self.exec_error}\n",
    "\n",
    "            Debugging Plan:\n",
    "            {plan}\n",
    "\n",
    "            Follow the Debugging Plan: Use the steps outlined in the debugging plan to guide your corrections.\n",
    "            \n",
    "\n",
    "            Output: Provide the final corrected Python code that is ready to be executed without errors.\n",
    "        \n",
    "        \"\"\"\n",
    "        return prompt\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        # Plan the work\n",
    "        planning_prompt = self.get_planning_prompt()\n",
    "        plan_response = self.get_response(planning_prompt)\n",
    "\n",
    "        # print(plan_response.output_text)\n",
    "        \n",
    "        # code generation\n",
    "        list_text = plan_response.output_text\n",
    "        code_gen_prompt = self.get_code_gen_prompt(list_text)\n",
    "        code_gen_response = self.get_response(code_gen_prompt)\n",
    "\n",
    "        extracted_code = extract_python_code(code_gen_response.output_text)\n",
    "\n",
    "        return(extracted_code)\n",
    "\n",
    "        # file_path = f\"./output/{self.thread_id}/model_training.py\"\n",
    "        # save_code(file_path, extracted_code[0])\n",
    "\n",
    "        # executor = PythonCodeExecutor()\n",
    "        # code = extracted_code[0]\n",
    "        # result = executor.execute(code)\n",
    "\n",
    "        # print(result.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "392106e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['import os \\nimport pandas as pd\\n\\ntry:\\n    df = pd.read_csv(\\'data/iris.csv\\')\\n    # Print the shape of the dataset\\n    print(f\"Dataset shape: {df.shape}\")\\nexcept FileNotFoundError:\\n    print(\"The file \\'data/iris.csv\\' was not found. Please check the file path.\")\\nexcept pd.errors.EmptyDataError:\\n    print(\"The file is empty. Please check the CSV file content.\")\\nexcept pd.errors.ParserError:\\n    print(\"Error parsing the CSV file. Please ensure it is formatted correctly.\")']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thread_id = 1\n",
    "task_description = \"write python code to print the shape of dataset iris.csv\"\n",
    "code = \"\"\"\n",
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/iris.csv')\n",
    "df.shape()\n",
    "\"\"\"\n",
    "exec_error = \"\"\"\n",
    "File \"/Users/prajwalchaudhary/Desktop/Uni/COMP8420/AutoAgentML/testing_codevef.py\", line 5, in <module>\n",
    "    df.shape()\n",
    "    ~~~~~~~~^^\n",
    "TypeError: 'tuple' object is not callable\n",
    "\"\"\"\n",
    "codevef = CodeVerifierAgent(thread_id, task_description, code, exec_error)\n",
    "codevef.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2d3ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
