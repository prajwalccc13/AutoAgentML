{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "396fa7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a4a0f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4.1-nano-2025-04-14\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "62d88003",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are an expert Data Scientist. Your task is to generate a step-by-step list of Exploratory Data Analysis (EDA) tasks tailored to the given data type, with a focus on supporting downstream agents for feature engineering, model training, and evaluation.\n",
    "\n",
    "Requirements:\n",
    "- All tasks should be designed so that their outputs (important textual or numeric summaries, statistics, or lists) are logged into a structured JSON file, not displayed.\n",
    "- Tasks must begin with data loading and proceed through all essential EDA steps, including identifying data types, missing values, statistical summaries, cardinality, outlier detection, and any domain-specific EDA needed for modeling.\n",
    "- Avoid tasks that only generate visualizations unless the underlying data/summary is also saved as JSON.\n",
    "- Each task should be written as a single string, achievable via Python, and focus on producing outputs that can be consumed programmatically by downstream agents.\n",
    "- Do not output code, explanations, or any text outside the Python list of task descriptions.\n",
    "\n",
    "Input:\n",
    "- Data type: csv\n",
    "\n",
    "Output format:\n",
    "A Python list of EDA task descriptions as strings, with each task specifically designed so its results are logged into a JSON file for use by downstream agents. Make sure to save all the results in the JSON file and all values to be logged are JSON serializable.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4a42af76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='[\\n    \"Load the CSV data into a pandas DataFrame and save the shape (rows, columns) as a dictionary in JSON with key \\'data_shape\\'.\",\\n    \"Identify data types for each column and save the resulting data type mapping as a JSON object with key \\'column_data_types\\'.\",\\n    \"Calculate the total number of missing values per column and save as a JSON dictionary with key \\'missing_values_counts\\'.\",\\n    \"Compute the percentage of missing values per column and save as a JSON dictionary with key \\'missing_values_percentage\\'.\",\\n    \"Generate descriptive statistical summaries (count, mean, std, min, 25%, 50%, 75%, max) for numerical columns and save as a JSON object with key \\'numerical_summary\\'.\",\\n    \"Generate descriptive statistical summaries (count, unique, top, freq) for categorical columns and save as a JSON object with key \\'categorical_summary\\'.\",\\n    \"Determine the number of unique values for each column and save as a JSON dictionary with key \\'column_unique_counts\\'.\",\\n    \"Identify the top 5 most frequent values for each categorical column and save as a JSON object with key \\'top_values_per_categorical_column\\'.\",\\n    \"Calculate the cardinality (number of unique values) for each column and save as a JSON dictionary with key \\'column_cardinality\\'.\",\\n    \"Detect outliers in numerical columns using the IQR method, and save the count and indices of outliers per numerical column as a JSON object with key \\'numerical_outliers\\'.\",\\n    \"Assess the distribution of numerical features by calculating skewness and kurtosis, and save as a JSON object with key \\'numerical_distribution_stats\\'.\",\\n    \"Identify potential multicollinearity by computing correlation coefficients between numerical features and save the correlation matrix as a JSON object with key \\'numerical_correlation_matrix\\'.\",\\n    \"For each categorical feature, compute the frequency distribution of categories and save as a JSON object with key \\'categorical_value_counts\\'.\",\\n    \"Identify and log the presence of duplicate rows in the dataset as a boolean and count, saved in JSON with keys \\'has_duplicates\\' and \\'duplicate_count\\'.\",\\n    \"Check for class imbalance if a target variable is specified; compute class distribution counts and save as a JSON object with key \\'target_class_distribution\\'.\",\\n    \"Summarize the data quality by logging the number of complete cases (rows without missing values) as an integer in JSON with key \\'complete_case_count\\'.\",\\n    \"Identify potential domain-specific anomalies or data issues based on known domain rules (e.g., negative ages, impossible values) and log findings as a JSON list under key \\'domain_anomalies\\'.\",\\n    \"Compile all collected summaries and statistics into a single JSON object and save to a file named \\'eda_results.json\\'.\"\\n]', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 567, 'prompt_tokens': 282, 'total_tokens': 849, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-BiDftZS7P2I0dyL7SfmVZMITFrIVO', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--dd076734-5342-45d4-a443-351c2d2b7452-0', usage_metadata={'input_tokens': 282, 'output_tokens': 567, 'total_tokens': 849, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are an expert Prompt Enginner.\",\n",
    "    ),\n",
    "    (\"human\", prompt),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5d36ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = ai_msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5a86f7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    \"Load the CSV dataset into a pandas DataFrame and save its shape (number of rows and columns) into the JSON log under 'dataset_shape'.\",\n",
      "    \"Identify the data types for each column in the DataFrame and save the data types dictionary into the JSON log under 'column_data_types'.\",\n",
      "    \"Compute the number and percentage of missing values for each column and save this information into the JSON log under 'missing_value_counts_and_percentages'.\",\n",
      "    \"Calculate basic statistical summaries (mean, median, min, max, standard deviation, quartiles) for all numeric columns and save the summary statistics into the JSON log under 'numeric_summary_statistics'.\",\n",
      "    \"Calculate basic statistical summaries (count, unique values, top, frequency) for all categorical columns and save the categorical summaries into the JSON log under 'categorical_summary_statistics'.\",\n",
      "    \"Determine the cardinality (number of unique values) for each column and save these counts into the JSON log under 'column_cardinalities'.\",\n",
      "    \"Identify columns with high missing value ratios (above a specified threshold, e.g., 50%) and save the list of such columns along with their missing ratios into the JSON log under 'high_missing_value_columns'.\",\n",
      "    \"Detect potential outliers in numeric columns using the Interquartile Range (IQR) method, and save the outlier counts and indices for each numeric column into the JSON log under 'numeric_outliers'.\",\n",
      "    \"For each categorical column, compute the frequency distribution of each category and save the top 5 most frequent categories with their counts into the JSON log under 'categorical_top_categories'.\",\n",
      "    \"Identify columns with low variance (e.g., less than 1% variance) in their numeric values and save their names into the JSON log under 'low_variance_numeric_columns'.\",\n",
      "    \"Summarize the distribution of target variables if present, including class counts for classification tasks, paginated to support class imbalance detection, and save into the JSON log under 'target_variable_distribution'.\",\n",
      "    \"Perform correlation analysis among numeric features using Pearson correlation coefficients, and save the correlation matrix as a nested list into the JSON log under 'numeric_feature_correlation_matrix'.\",\n",
      "    \"Generate a list of feature columns that are highly correlated (above a threshold, e.g., 0.8 or below -0.8) with the target variable, saved under 'highly_correlated_features_with_target'.\",\n",
      "    \"Summarize and log any domain-specific data quality issues observed during EDA, such as duplicated entries, inconsistent categories, or suspicious patterns, under 'data_quality_notes'.\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1-nano-2025-04-14\",\n",
    "    input=prompt\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9b2a3098",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_text = response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8d4b4884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Certainly! Below is a comprehensive Python script that performs all the listed data analysis tasks on a CSV dataset, saving the results into a JSON log. You need to specify the CSV filename and optionally the target variable name (for classification tasks), as well as thresholds like missing value ratio, correlation cutoff, and variance cutoff.\\n\\n```python\\nimport pandas as pd\\nimport json\\nimport numpy as np\\n\\n# Configuration\\nCSV_FILE_PATH = \\'your_dataset.csv\\'  # Replace with your CSV file path\\nJSON_LOG_PATH = \\'data_analysis_log.json\\'\\nTARGET_VARIABLE = \\'target\\'  # Set to None if no target variable\\nMISSING_VALUE_THRESHOLD = 0.5  # 50%\\nCORRELATION_THRESHOLD = 0.8\\nLOW_VARIANCE_THRESHOLD = 0.01  # 1%\\n\\n# Load dataset\\ndf = pd.read_csv(CSV_FILE_PATH)\\n\\n# Initialize log dictionary\\nlog = {}\\n\\n# 1. Dataset shape\\nlog[\\'dataset_shape\\'] = df.shape\\n\\n# 2. Data types\\nlog[\\'column_data_types\\'] = df.dtypes.astype(str).to_dict()\\n\\n# 3. Missing values\\nmissing_counts = df.isnull().sum()\\nmissing_percentages = (missing_counts / len(df))\\nlog[\\'missing_value_counts_and_percentages\\'] = {\\n    col: {\\n        \\'missing_count\\': int(count),\\n        \\'missing_percentage\\': float(perc)\\n    }\\n    for col, count, perc in zip(df.columns, missing_counts, missing_percentages)\\n}\\n\\n# 4. Numeric summary statistics\\nnumeric_cols = df.select_dtypes(include=[np.number]).columns\\nnumeric_summary = df[numeric_cols].describe().T\\n# Add median separately\\nnumeric_summary[\\'median\\'] = df[numeric_cols].median()\\nlog[\\'numeric_summary_statistics\\'] = numeric_summary.to_dict()\\n\\n# 5. Categorical summary statistics\\ncategorical_cols = df.select_dtypes(include=[\\'object\\', \\'category\\']).columns\\ncategorical_summary = {}\\nfor col in categorical_cols:\\n    value_counts = df[col].value_counts(dropna=False)\\n    categorical_summary[col] = {\\n        \\'count\\': value_counts.to_dict(),\\n        \\'unique\\': df[col].nunique(),\\n        \\'top\\': value_counts.idxmax(),\\n        \\'top_freq\\': int(value_counts.max())\\n    }\\nlog[\\'categorical_summary_statistics\\'] = categorical_summary\\n\\n# 6. Column cardinalities\\ncolumn_cardinalities = df.nunique()\\nlog[\\'column_cardinalities\\'] = column_cardinalities.to_dict()\\n\\n# 7. Columns with high missing ratio\\nhigh_missing_cols = missing_percentages[missing_percentages > MISSING_VALUE_THRESHOLD]\\nlog[\\'high_missing_value_columns\\'] = {\\n    col: float(perc)\\n    for col, perc in high_missing_cols.items()\\n}\\n\\n# 8. Outliers in numeric columns (IQR method)\\nnumeric_outliers = {}\\nfor col in numeric_cols:\\n    Q1 = df[col].quantile(0.25)\\n    Q3 = df[col].quantile(0.75)\\n    IQR = Q3 - Q1\\n    lower_bound = Q1 - 1.5 * IQR\\n    upper_bound = Q3 + 1.5 * IQR\\n    outlier_mask = (df[col] < lower_bound) | (df[col] > upper_bound)\\n    outlier_indices = df.index[outlier_mask].tolist()\\n    numeric_outliers[col] = {\\n        \\'outlier_count\\': int(outlier_mask.sum()),\\n        \\'outlier_indices\\': outlier_indices\\n    }\\nlog[\\'numeric_outliers\\'] = numeric_outliers\\n\\n# 9. Top 5 categories per categorical column\\ncategorical_top_categories = {}\\nfor col in categorical_cols:\\n    value_counts = df[col].value_counts().head(5)\\n    categorical_top_categories[col] = value_counts.to_dict()\\nlog[\\'categorical_top_categories\\'] = categorical_top_categories\\n\\n# 10. Low variance numeric columns\\nlow_variance_cols = []\\nfor col in numeric_cols:\\n    if df[col].var() < LOW_VARIANCE_THRESHOLD:\\n        low_variance_cols.append(col)\\nlog[\\'low_variance_numeric_columns\\'] = low_variance_cols\\n\\n# 11. Target variable distribution (if specified)\\nif TARGET_VARIABLE and TARGET_VARIABLE in df.columns:\\n    target_counts = df[TARGET_VARIABLE].value_counts()\\n    # For classification, show class counts; for regression, maybe stats\\n    log[\\'target_variable_distribution\\'] = target_counts.to_dict()\\n\\n# 12. Correlation matrix of numeric features\\ncorrelation_matrix = df[numeric_cols].corr(method=\\'pearson\\')\\n# Convert to nested list\\nlog[\\'numeric_feature_correlation_matrix\\'] = correlation_matrix.values.tolist()\\n\\n# 13. Highly correlated features with target\\nhigh_corr_features = []\\nif TARGET_VARIABLE and TARGET_VARIABLE in df.columns:\\n    target_corrs = correlation_matrix[TARGET_VARIABLE]\\n    for col in numeric_cols:\\n        if col != TARGET_VARIABLE:\\n            corr_value = target_corrs[col]\\n            if abs(corr_value) > CORRELATION_THRESHOLD:\\n                high_corr_features.append(col)\\nlog[\\'highly_correlated_features_with_target\\'] = high_corr_features\\n\\n# 14. Domain-specific data quality notes (manual or heuristic)\\n# For illustration, let\\'s check for duplicated entries and suspicious patterns\\nduplicates = df.duplicated()\\ndata_quality_notes = []\\nif duplicates.any():\\n    data_quality_notes.append(f\"Found {duplicates.sum()} duplicated rows.\")\\n# Check for inconsistent categories (e.g., presence of unexpected categories)\\n# Example (can be customized)\\n# For simplicity, assume no specific rules; expand as needed\\nlog[\\'data_quality_notes\\'] = data_quality_notes\\n\\n# Save the log to JSON\\nwith open(JSON_LOG_PATH, \\'w\\') as f:\\n    json.dump(log, f, indent=4)\\n\\nprint(f\"Analysis complete. Log saved to {JSON_LOG_PATH}.\")\\n```\\n\\n### How to use:\\n\\n1. Replace `\\'your_dataset.csv\\'` with the actual path to your dataset.\\n2. Set `TARGET_VARIABLE` if your dataset has a target column.\\n3. Adjust thresholds if needed.\\n4. Run the script.\\n\\nThis will generate a detailed JSON report covering all the requested analyses.\\n\\nLet me know if you\\'d like a specific part expanded or customized!'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "        model=\"gpt-4.1-nano-2025-04-14\",\n",
    "        input= f\"write python code for {list_text}\"\n",
    "    )\n",
    "response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d2756499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_python_code(text):\n",
    "    \"\"\"\n",
    "    Extracts Python code blocks enclosed in ```python ... ``` from a given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing the code.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of extracted Python code strings.\n",
    "    \"\"\"\n",
    "    # The regex looks for:\n",
    "    # ```python          - literal start of the code block marker\n",
    "    # \\s* - optional whitespace (e.g., newline)\n",
    "    # (.*?)              - non-greedy capture of any character (the code itself)\n",
    "    # \\s* - optional whitespace before the closing marker\n",
    "    # ```                - literal end of the code block marker\n",
    "    python_code_blocks = re.findall(r\"```python\\s*(.*?)\\s*```\", text, re.DOTALL)\n",
    "    return [block.strip() for block in python_code_blocks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9698cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_code = extract_python_code(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4764049a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nimport json\\nimport numpy as np\\n\\n# Configuration\\nCSV_FILE_PATH = \\'your_dataset.csv\\'  # Replace with your CSV file path\\nJSON_LOG_PATH = \\'data_analysis_log.json\\'\\nTARGET_VARIABLE = \\'target\\'  # Set to None if no target variable\\nMISSING_VALUE_THRESHOLD = 0.5  # 50%\\nCORRELATION_THRESHOLD = 0.8\\nLOW_VARIANCE_THRESHOLD = 0.01  # 1%\\n\\n# Load dataset\\ndf = pd.read_csv(CSV_FILE_PATH)\\n\\n# Initialize log dictionary\\nlog = {}\\n\\n# 1. Dataset shape\\nlog[\\'dataset_shape\\'] = df.shape\\n\\n# 2. Data types\\nlog[\\'column_data_types\\'] = df.dtypes.astype(str).to_dict()\\n\\n# 3. Missing values\\nmissing_counts = df.isnull().sum()\\nmissing_percentages = (missing_counts / len(df))\\nlog[\\'missing_value_counts_and_percentages\\'] = {\\n    col: {\\n        \\'missing_count\\': int(count),\\n        \\'missing_percentage\\': float(perc)\\n    }\\n    for col, count, perc in zip(df.columns, missing_counts, missing_percentages)\\n}\\n\\n# 4. Numeric summary statistics\\nnumeric_cols = df.select_dtypes(include=[np.number]).columns\\nnumeric_summary = df[numeric_cols].describe().T\\n# Add median separately\\nnumeric_summary[\\'median\\'] = df[numeric_cols].median()\\nlog[\\'numeric_summary_statistics\\'] = numeric_summary.to_dict()\\n\\n# 5. Categorical summary statistics\\ncategorical_cols = df.select_dtypes(include=[\\'object\\', \\'category\\']).columns\\ncategorical_summary = {}\\nfor col in categorical_cols:\\n    value_counts = df[col].value_counts(dropna=False)\\n    categorical_summary[col] = {\\n        \\'count\\': value_counts.to_dict(),\\n        \\'unique\\': df[col].nunique(),\\n        \\'top\\': value_counts.idxmax(),\\n        \\'top_freq\\': int(value_counts.max())\\n    }\\nlog[\\'categorical_summary_statistics\\'] = categorical_summary\\n\\n# 6. Column cardinalities\\ncolumn_cardinalities = df.nunique()\\nlog[\\'column_cardinalities\\'] = column_cardinalities.to_dict()\\n\\n# 7. Columns with high missing ratio\\nhigh_missing_cols = missing_percentages[missing_percentages > MISSING_VALUE_THRESHOLD]\\nlog[\\'high_missing_value_columns\\'] = {\\n    col: float(perc)\\n    for col, perc in high_missing_cols.items()\\n}\\n\\n# 8. Outliers in numeric columns (IQR method)\\nnumeric_outliers = {}\\nfor col in numeric_cols:\\n    Q1 = df[col].quantile(0.25)\\n    Q3 = df[col].quantile(0.75)\\n    IQR = Q3 - Q1\\n    lower_bound = Q1 - 1.5 * IQR\\n    upper_bound = Q3 + 1.5 * IQR\\n    outlier_mask = (df[col] < lower_bound) | (df[col] > upper_bound)\\n    outlier_indices = df.index[outlier_mask].tolist()\\n    numeric_outliers[col] = {\\n        \\'outlier_count\\': int(outlier_mask.sum()),\\n        \\'outlier_indices\\': outlier_indices\\n    }\\nlog[\\'numeric_outliers\\'] = numeric_outliers\\n\\n# 9. Top 5 categories per categorical column\\ncategorical_top_categories = {}\\nfor col in categorical_cols:\\n    value_counts = df[col].value_counts().head(5)\\n    categorical_top_categories[col] = value_counts.to_dict()\\nlog[\\'categorical_top_categories\\'] = categorical_top_categories\\n\\n# 10. Low variance numeric columns\\nlow_variance_cols = []\\nfor col in numeric_cols:\\n    if df[col].var() < LOW_VARIANCE_THRESHOLD:\\n        low_variance_cols.append(col)\\nlog[\\'low_variance_numeric_columns\\'] = low_variance_cols\\n\\n# 11. Target variable distribution (if specified)\\nif TARGET_VARIABLE and TARGET_VARIABLE in df.columns:\\n    target_counts = df[TARGET_VARIABLE].value_counts()\\n    # For classification, show class counts; for regression, maybe stats\\n    log[\\'target_variable_distribution\\'] = target_counts.to_dict()\\n\\n# 12. Correlation matrix of numeric features\\ncorrelation_matrix = df[numeric_cols].corr(method=\\'pearson\\')\\n# Convert to nested list\\nlog[\\'numeric_feature_correlation_matrix\\'] = correlation_matrix.values.tolist()\\n\\n# 13. Highly correlated features with target\\nhigh_corr_features = []\\nif TARGET_VARIABLE and TARGET_VARIABLE in df.columns:\\n    target_corrs = correlation_matrix[TARGET_VARIABLE]\\n    for col in numeric_cols:\\n        if col != TARGET_VARIABLE:\\n            corr_value = target_corrs[col]\\n            if abs(corr_value) > CORRELATION_THRESHOLD:\\n                high_corr_features.append(col)\\nlog[\\'highly_correlated_features_with_target\\'] = high_corr_features\\n\\n# 14. Domain-specific data quality notes (manual or heuristic)\\n# For illustration, let\\'s check for duplicated entries and suspicious patterns\\nduplicates = df.duplicated()\\ndata_quality_notes = []\\nif duplicates.any():\\n    data_quality_notes.append(f\"Found {duplicates.sum()} duplicated rows.\")\\n# Check for inconsistent categories (e.g., presence of unexpected categories)\\n# Example (can be customized)\\n# For simplicity, assume no specific rules; expand as needed\\nlog[\\'data_quality_notes\\'] = data_quality_notes\\n\\n# Save the log to JSON\\nwith open(JSON_LOG_PATH, \\'w\\') as f:\\n    json.dump(log, f, indent=4)\\n\\nprint(f\"Analysis complete. Log saved to {JSON_LOG_PATH}.\")'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_code[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "578071d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code successfully saved to 'my_script-gpt.py'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Define the filename for the Python script\n",
    "filename = \"my_script-gpt.py\"\n",
    "\n",
    "try:\n",
    "    # Save the code to a file\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(extracted_code[0])\n",
    "    print(f\"Code successfully saved to '{filename}'\")\n",
    "\n",
    "except IOError as e:\n",
    "    print(f\"An error occurred while writing to the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c2ffd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "742e7111",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8774a12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# --- Configuration ---\n",
    "# The path to your JSON file containing the data description.\n",
    "# Make sure this file exists in the same directory, or provide a full path.\n",
    "json_file_path = 'data_analysis_log.json'\n",
    "\n",
    "prompt_template = f\"\"\"\n",
    "You are an expert Machine Learning Engineer. Your task is to generate a step-by-step list of tasks for training and evaluating a machine learning model using structured tabular data.\n",
    "\n",
    "Context:\n",
    "- The input data is in CSV format.\n",
    "- A previous agent (EDA agent) has already analyzed the dataset and generated a structured JSON file named {json_file_path}. This file contains outputs such as data types, missing value statistics, cardinality, outlier info, and distribution summaries.\n",
    "\n",
    "Requirements:\n",
    "- All tasks should use insights from the EDA JSON file ({json_file_path}) where appropriate.\n",
    "- Each task should produce outputs (such as selected features, cleaned dataset paths, model hyperparameters, evaluation metrics, or model file paths) that are JSON-serializable and must be logged into a structured JSON file. This JSON will be consumed by downstream agents for deployment, explanation, or monitoring.\n",
    "- Tasks must begin with reading the EDA output and continue through preprocessing, feature selection, train-test splitting, model training, evaluation, and saving of final artifacts.\n",
    "- Focus on general-purpose supervised machine learning (classification or regression depending on the target variable).\n",
    "- Avoid any tasks that only generate visualizations unless their summaries or values are saved in structured form.\n",
    "- Each task must be expressed as a single string that could be executed in Python and designed to run sequentially.\n",
    "- Do not output code, explanations, or any text outside the Python list of task descriptions.\n",
    "\n",
    "Input:\n",
    "- Data type: csv\n",
    "- EDA results: structured JSON file \n",
    "\n",
    "Output format:\n",
    "A Python list of model training task descriptions as strings, with each task specifically designed so its results are logged into a structured JSON file for use by downstream agents.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fa4cb9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    \"Read 'data_analysis_log.json' to extract insights on data types, missing values, outliers, and feature distributions.\",\n",
      "    \"Load the raw dataset from 'dataset.csv' into a pandas DataFrame.\",\n",
      "    \"Identify and handle missing values: for numerical features, fill missing values with median; for categorical features, fill with mode, and log the counts in 'preprocessing.json'.\",\n",
      "    \"Remove or treat outliers as indicated in 'data_analysis_log.json' (e.g., capping or filtering), and record applied methods and remaining outlier counts in 'preprocessing.json'.\",\n",
      "    \"Encode categorical features: apply one-hot encoding or label encoding based on cardinality insights, and save the number of categories per feature in 'preprocessing.json'.\",\n",
      "    \"Normalize or scale numerical features according to identified distributions: apply MinMaxScaler or StandardScaler, and log the scaler parameters in 'preprocessing.json'.\",\n",
      "    \"Select features for modeling based on EDA insights: include features with high correlation to target or low multicollinearity, and record selected feature list in 'feature_selection.json'.\",\n",
      "    \"Split the dataset into training and testing sets (e.g., 80-20), stratified if classification, and store split indices or sizes in 'train_test_split.json'.\",\n",
      "    \"Define a machine learning model pipeline (e.g., scaler + classifier/regressor) with chosen hyperparameters; tune hyperparameters via grid search or random search, and log the hyperparameters and best score in 'optimization.json'.\",\n",
      "    \"Train the model pipeline on the training data, save the trained model to disk (e.g., 'model.pkl'), and record the model path and training metadata in 'model_training.json'.\",\n",
      "    \"Evaluate the trained model on the test set: generate predictions, compute evaluation metrics (accuracy, precision, recall, F1 for classification; RMSE, R2 for regression), and save these metrics in 'evaluation.json'.\",\n",
      "    \"Save the final processed dataset (training and test) to disk for downstream validation, and include their file paths in 'preprocessed_datasets.json'.\",\n",
      "    \"Collect all results, including feature selection, preprocessing steps, model parameters, evaluation metrics, and file paths into a comprehensive summary JSON file for downstream agents.\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1-nano-2025-04-14\",\n",
    "    input=prompt_template\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a741ac41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Certainly! Here's a comprehensive Python script implementing the outlined steps. Note that you'll need to adjust file paths, feature names, target variable, and model choices based on your specific dataset.\\n\\n```python\\nimport json\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.model_selection import train_test_split, GridSearchCV\\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.ensemble import RandomForestClassifier  # or regressor\\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, r2_score\\nimport joblib\\n\\n# Load previous data analysis log\\nwith open('data_analysis_log.json', 'r') as f:\\n    analysis_log = json.load(f)\\n\\n# Load dataset\\ndf = pd.read_csv('dataset.csv')\\n\\n# Initialize logs\\npreprocessing_log = {}\\nfeature_selection_log = {}\\nsplit_log = {}\\noptimization_log = {}\\nmodel_training_log = {}\\nevaluation_log = {}\\npreprocessed_files = {}\\nfinal_dataset_files = {}\\ncomprehensive_summary = {}\\n\\n# Identify feature types\\nnumeric_features = df.select_dtypes(include=['number']).columns.tolist()\\ncategorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()\\n\\n# Handle missing values\\n# Numerical: median; Categorical: mode\\nnum_imputer = SimpleImputer(strategy='median')\\ncat_imputer = SimpleImputer(strategy='most_frequent')\\n\\ndf[numeric_features] = num_imputer.fit_transform(df[numic_features]) if numeric_features else df[numeric_features]\\ndf[categorical_features] = cat_imputer.fit_transform(df[categorical_features]) if categorical_features else df[categorical_features]\\n\\n# Log missing values count\\npreprocessing_log['missing_values'] = {\\n    'numeric': {feat: df[feat].isnull().sum() for feat in numeric_features},\\n    'categorical': {feat: df[feat].isnull().sum() for feat in categorical_features}\\n}\\n\\n# Outlier treatment based on analysis log\\n# Example: capping outliers at 1.5*IQR\\nfor feature in numeric_features:\\n    if analysis_log['outliers'].get(feature, {}).get('handle', '') == 'cap':\\n        Q1 = df[feature].quantile(0.25)\\n        Q3 = df[feature].quantile(0.75)\\n        IQR = Q3 - Q1\\n        lower_bound = Q1 - 1.5 * IQR\\n        upper_bound = Q3 + 1.5 * IQR\\n        df[feature] = df[feature].clip(lower=lower_bound, upper=upper_bound)\\n\\n# Log outlier treatment\\npreprocessing_log['outlier_removal'] = {feature: 'capped at 1.5*IQR' for feature in numeric_features}\\n\\n# Encode categorical features\\n# Decide encoding based on cardinality\\ncat_encoding = {}\\nfor feat in categorical_features:\\n    unique_vals = df[feat].nunique()\\n    if unique_vals <= 10:\\n        # Use one-hot\\n        encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\\n        encoded = encoder.fit_transform(df[[feat]])\\n        encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out([feat]))\\n        df = pd.concat([df.drop(columns=[feat]), encoded_df], axis=1)\\n        cat_encoding[feat] = {'method': 'onehot', 'categories': encoder.categories_[0]}\\n    else:\\n        # Use label encoding\\n        le = LabelEncoder()\\n        df[feat] = le.fit_transform(df[feat])\\n        cat_encoding[feat] = {'method': 'label', 'categories': le.classes_}\\n\\n# Record encoding info\\npreprocessing_log['categorical_encoding'] = cat_encoding\\n\\n# Normalize/scaling numerical features\\ndistribution_info = analysis_log.get('feature_distributions', {})\\nscalers_params = {}\\nfor feature in numeric_features:\\n    dist_type = distribution_info.get(feature, {}).get('type', '')\\n    if dist_type == 'normal':\\n        scaler = StandardScaler()\\n        df[feature] = scaler.fit_transform(df[[feature]])\\n        scalers_params[feature] = {'mean': scaler.mean_[0], 'scale': scaler.scale_[0]}\\n    else:\\n        scaler = MinMaxScaler()\\n        df[feature] = scaler.fit_transform(df[[feature]])\\n        scalers_params[feature] = {'min': scaler.data_min_[0], 'max': scaler.data_max_[0]}\\n\\n# Record scaler parameters\\npreprocessing_log['numerical_scaling'] = scalers_params\\n\\n# Define features and target\\ntarget_variable = 'target'  # replace with your target column name\\nX = df.drop(columns=[target_variable])\\ny = df[target_variable]\\n\\n# Feature selection based on correlation or multicollinearity\\n# Example: highly correlated with target or low multicollinearity\\n# Placeholder: select features with correlation > threshold\\ncorr_threshold = 0.5\\ncorrs = X.corrwith(y).abs()\\nselected_features = corrs[corrs > corr_threshold].index.tolist()\\n\\n# Note: For multicollinearity, compute VIFs and remove variables accordingly\\nfeature_selection_log['selected_features'] = selected_features\\n\\n# Split dataset\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, y, test_size=0.2, stratify=y if y.nunique() <= 20 else None, random_state=42\\n)\\n\\nsplit_log['train_size'] = len(X_train)\\nsplit_log['test_size'] = len(X_test)\\n\\n# Save split info\\nwith open('train_test_split.json', 'w') as f:\\n    json.dump(split_log, f, indent=4)\\n\\n# Define ML pipeline\\npipeline = Pipeline(steps=[\\n    # Already scaled and encoded features if applicable; if not, include scalers here using ColumnTransformer\\n    ('classifier', RandomForestClassifier(random_state=42))\\n])\\n\\n# Hyperparameter tuning\\nparam_grid = {\\n    'classifier__n_estimators': [50, 100],\\n    'classifier__max_depth': [None, 10, 20],\\n}\\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\\ngrid_search.fit(X_train[selected_features], y_train)\\n\\n# Log hyperparameters and best score\\noptimization_log['best_params'] = grid_search.best_params_\\noptimization_log['best_score'] = grid_search.best_score_\\n\\n# Save optimization details\\nwith open('optimization.json', 'w') as f:\\n    json.dump(optimization_log, f, indent=4)\\n\\n# Train final model\\nbest_model = grid_search.best_estimator_\\nmodel_path = 'model.pkl'\\njoblib.dump(best_model, model_path)\\nmodel_training_log['model_path'] = model_path\\nmodel_training_log['training_time'] = 'placeholder'  # Could add actual timing info\\n\\n# Save trained model info\\nwith open('model_training.json', 'w') as f:\\n    json.dump(model_training_log, f, indent=4)\\n\\n# Evaluate on test set\\ny_pred = best_model.predict(X_test[selected_features])\\n\\n# Compute metrics\\naccuracy = accuracy_score(y_test, y_pred)\\nprecision = precision_score(y_test, y_pred, average='weighted')\\nrecall = recall_score(y_test, y_pred, average='weighted')\\nf1 = f1_score(y_test, y_pred, average='weighted')\\n\\nevaluation_log['classification_metrics'] = {\\n    'accuracy': accuracy,\\n    'precision': precision,\\n    'recall': recall,\\n    'f1_score': f1\\n}\\n\\n# Save evaluation metrics\\nwith open('evaluation.json', 'w') as f:\\n    json.dump(evaluation_log, f, indent=4)\\n\\n# Save processed datasets\\ntrain_file = 'processed_train.csv'\\ntest_file = 'processed_test.csv'\\nX_train.to_csv(train_file, index=False)\\nX_test.to_csv(test_file, index=False)\\npreprocessed_files['train'] = train_file\\npreprocessed_files['test'] = test_file\\n\\n# Save final dataset file paths\\nwith open('preprocessed_datasets.json', 'w') as f:\\n    json.dump(preprocessed_files, f, indent=4)\\n\\n# Compile all results into a comprehensive summary\\ncomprehensive_summary = {\\n    'feature_selection': feature_selection_log,\\n    'preprocessing': preprocessing_log,\\n    'hyperparameter_optimization': optimization_log,\\n    'model_training': model_training_log,\\n    'evaluation': evaluation_log,\\n    'dataset_files': preprocessed_files\\n}\\n\\n# Save comprehensive summary\\nwith open('full_summary.json', 'w') as f:\\n    json.dump(comprehensive_summary, f, indent=4)\\n```\\n\\n**Notes:**\\n- Replace `'target'` with your actual target variable name.\\n- Adjust outlier treatment, feature encoding strategy, scaling, and model choice per your dataset.\\n- For hyperparameter tuning, configure the `param_grid`.\\n- Timing info, detailed logs, and exceptions should be added for production robustness.\\n- Ensure the necessary libraries are installed (`scikit-learn`, `pandas`, `numpy`, `joblib`).\\n\\nThis script provides a structured pipeline aligning with your specifications.\""
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_text = response.output_text\n",
    "response = client.responses.create(\n",
    "        model=\"gpt-4.1-nano-2025-04-14\",\n",
    "        input= f\"write python code for {list_text}\"\n",
    "    )\n",
    "response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bd798f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import json\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.model_selection import train_test_split, GridSearchCV\\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.ensemble import RandomForestClassifier  # or regressor\\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, r2_score\\nimport joblib\\n\\n# Load previous data analysis log\\nwith open('data_analysis_log.json', 'r') as f:\\n    analysis_log = json.load(f)\\n\\n# Load dataset\\ndf = pd.read_csv('dataset.csv')\\n\\n# Initialize logs\\npreprocessing_log = {}\\nfeature_selection_log = {}\\nsplit_log = {}\\noptimization_log = {}\\nmodel_training_log = {}\\nevaluation_log = {}\\npreprocessed_files = {}\\nfinal_dataset_files = {}\\ncomprehensive_summary = {}\\n\\n# Identify feature types\\nnumeric_features = df.select_dtypes(include=['number']).columns.tolist()\\ncategorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()\\n\\n# Handle missing values\\n# Numerical: median; Categorical: mode\\nnum_imputer = SimpleImputer(strategy='median')\\ncat_imputer = SimpleImputer(strategy='most_frequent')\\n\\ndf[numeric_features] = num_imputer.fit_transform(df[numic_features]) if numeric_features else df[numeric_features]\\ndf[categorical_features] = cat_imputer.fit_transform(df[categorical_features]) if categorical_features else df[categorical_features]\\n\\n# Log missing values count\\npreprocessing_log['missing_values'] = {\\n    'numeric': {feat: df[feat].isnull().sum() for feat in numeric_features},\\n    'categorical': {feat: df[feat].isnull().sum() for feat in categorical_features}\\n}\\n\\n# Outlier treatment based on analysis log\\n# Example: capping outliers at 1.5*IQR\\nfor feature in numeric_features:\\n    if analysis_log['outliers'].get(feature, {}).get('handle', '') == 'cap':\\n        Q1 = df[feature].quantile(0.25)\\n        Q3 = df[feature].quantile(0.75)\\n        IQR = Q3 - Q1\\n        lower_bound = Q1 - 1.5 * IQR\\n        upper_bound = Q3 + 1.5 * IQR\\n        df[feature] = df[feature].clip(lower=lower_bound, upper=upper_bound)\\n\\n# Log outlier treatment\\npreprocessing_log['outlier_removal'] = {feature: 'capped at 1.5*IQR' for feature in numeric_features}\\n\\n# Encode categorical features\\n# Decide encoding based on cardinality\\ncat_encoding = {}\\nfor feat in categorical_features:\\n    unique_vals = df[feat].nunique()\\n    if unique_vals <= 10:\\n        # Use one-hot\\n        encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\\n        encoded = encoder.fit_transform(df[[feat]])\\n        encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out([feat]))\\n        df = pd.concat([df.drop(columns=[feat]), encoded_df], axis=1)\\n        cat_encoding[feat] = {'method': 'onehot', 'categories': encoder.categories_[0]}\\n    else:\\n        # Use label encoding\\n        le = LabelEncoder()\\n        df[feat] = le.fit_transform(df[feat])\\n        cat_encoding[feat] = {'method': 'label', 'categories': le.classes_}\\n\\n# Record encoding info\\npreprocessing_log['categorical_encoding'] = cat_encoding\\n\\n# Normalize/scaling numerical features\\ndistribution_info = analysis_log.get('feature_distributions', {})\\nscalers_params = {}\\nfor feature in numeric_features:\\n    dist_type = distribution_info.get(feature, {}).get('type', '')\\n    if dist_type == 'normal':\\n        scaler = StandardScaler()\\n        df[feature] = scaler.fit_transform(df[[feature]])\\n        scalers_params[feature] = {'mean': scaler.mean_[0], 'scale': scaler.scale_[0]}\\n    else:\\n        scaler = MinMaxScaler()\\n        df[feature] = scaler.fit_transform(df[[feature]])\\n        scalers_params[feature] = {'min': scaler.data_min_[0], 'max': scaler.data_max_[0]}\\n\\n# Record scaler parameters\\npreprocessing_log['numerical_scaling'] = scalers_params\\n\\n# Define features and target\\ntarget_variable = 'target'  # replace with your target column name\\nX = df.drop(columns=[target_variable])\\ny = df[target_variable]\\n\\n# Feature selection based on correlation or multicollinearity\\n# Example: highly correlated with target or low multicollinearity\\n# Placeholder: select features with correlation > threshold\\ncorr_threshold = 0.5\\ncorrs = X.corrwith(y).abs()\\nselected_features = corrs[corrs > corr_threshold].index.tolist()\\n\\n# Note: For multicollinearity, compute VIFs and remove variables accordingly\\nfeature_selection_log['selected_features'] = selected_features\\n\\n# Split dataset\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, y, test_size=0.2, stratify=y if y.nunique() <= 20 else None, random_state=42\\n)\\n\\nsplit_log['train_size'] = len(X_train)\\nsplit_log['test_size'] = len(X_test)\\n\\n# Save split info\\nwith open('train_test_split.json', 'w') as f:\\n    json.dump(split_log, f, indent=4)\\n\\n# Define ML pipeline\\npipeline = Pipeline(steps=[\\n    # Already scaled and encoded features if applicable; if not, include scalers here using ColumnTransformer\\n    ('classifier', RandomForestClassifier(random_state=42))\\n])\\n\\n# Hyperparameter tuning\\nparam_grid = {\\n    'classifier__n_estimators': [50, 100],\\n    'classifier__max_depth': [None, 10, 20],\\n}\\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\\ngrid_search.fit(X_train[selected_features], y_train)\\n\\n# Log hyperparameters and best score\\noptimization_log['best_params'] = grid_search.best_params_\\noptimization_log['best_score'] = grid_search.best_score_\\n\\n# Save optimization details\\nwith open('optimization.json', 'w') as f:\\n    json.dump(optimization_log, f, indent=4)\\n\\n# Train final model\\nbest_model = grid_search.best_estimator_\\nmodel_path = 'model.pkl'\\njoblib.dump(best_model, model_path)\\nmodel_training_log['model_path'] = model_path\\nmodel_training_log['training_time'] = 'placeholder'  # Could add actual timing info\\n\\n# Save trained model info\\nwith open('model_training.json', 'w') as f:\\n    json.dump(model_training_log, f, indent=4)\\n\\n# Evaluate on test set\\ny_pred = best_model.predict(X_test[selected_features])\\n\\n# Compute metrics\\naccuracy = accuracy_score(y_test, y_pred)\\nprecision = precision_score(y_test, y_pred, average='weighted')\\nrecall = recall_score(y_test, y_pred, average='weighted')\\nf1 = f1_score(y_test, y_pred, average='weighted')\\n\\nevaluation_log['classification_metrics'] = {\\n    'accuracy': accuracy,\\n    'precision': precision,\\n    'recall': recall,\\n    'f1_score': f1\\n}\\n\\n# Save evaluation metrics\\nwith open('evaluation.json', 'w') as f:\\n    json.dump(evaluation_log, f, indent=4)\\n\\n# Save processed datasets\\ntrain_file = 'processed_train.csv'\\ntest_file = 'processed_test.csv'\\nX_train.to_csv(train_file, index=False)\\nX_test.to_csv(test_file, index=False)\\npreprocessed_files['train'] = train_file\\npreprocessed_files['test'] = test_file\\n\\n# Save final dataset file paths\\nwith open('preprocessed_datasets.json', 'w') as f:\\n    json.dump(preprocessed_files, f, indent=4)\\n\\n# Compile all results into a comprehensive summary\\ncomprehensive_summary = {\\n    'feature_selection': feature_selection_log,\\n    'preprocessing': preprocessing_log,\\n    'hyperparameter_optimization': optimization_log,\\n    'model_training': model_training_log,\\n    'evaluation': evaluation_log,\\n    'dataset_files': preprocessed_files\\n}\\n\\n# Save comprehensive summary\\nwith open('full_summary.json', 'w') as f:\\n    json.dump(comprehensive_summary, f, indent=4)\""
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_code = extract_python_code(response.output_text)\n",
    "extracted_code[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6ee692d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code successfully saved to 'model_training_gpt.py'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Define the filename for the Python script\n",
    "filename = \"model_training_gpt.py\"\n",
    "\n",
    "try:\n",
    "    # Save the code to a file\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(extracted_code[0])\n",
    "    print(f\"Code successfully saved to '{filename}'\")\n",
    "\n",
    "except IOError as e:\n",
    "    print(f\"An error occurred while writing to the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4b7854f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ExecutionResult:\n",
    "    success: bool\n",
    "    stdout: str\n",
    "    stderr: str\n",
    "\n",
    "class PythonCodeExecutor:\n",
    "    def __init__(self, timeout: int = 10):\n",
    "        self.timeout = timeout\n",
    "\n",
    "    def execute(self, code: str) -> ExecutionResult:\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as temp_file:\n",
    "            temp_file.write(code)\n",
    "            temp_file_path = temp_file.name\n",
    "\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                ['python', temp_file_path],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            return ExecutionResult(\n",
    "                success=result.returncode == 0,\n",
    "                stdout=result.stdout,\n",
    "                stderr=result.stderr\n",
    "            )\n",
    "        except subprocess.TimeoutExpired as e:\n",
    "            return ExecutionResult(success=False, stdout='', stderr='Execution timed out.')\n",
    "        except Exception as e:\n",
    "            return ExecutionResult(success=False, stdout='', stderr=str(e))\n",
    "        finally:\n",
    "            os.remove(temp_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "382475f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Setosa       1.00      1.00      1.00        10\n",
      "  Versicolor       1.00      0.90      0.95        10\n",
      "   Virginica       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Setosa       1.00      1.00      1.00        10\n",
      "  Versicolor       1.00      0.90      0.95        10\n",
      "   Virginica       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "Gradient Boosting Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Setosa       1.00      1.00      1.00        10\n",
      "  Versicolor       1.00      0.90      0.95        10\n",
      "   Virginica       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "\n",
      "STDERR: \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    executor = PythonCodeExecutor()\n",
    "    code = extracted_code[0]\n",
    "    result = executor.execute(code)\n",
    "    print(\"STDOUT:\", result.stdout)\n",
    "    print(\"STDERR:\", result.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290d457c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
