{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "396fa7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a4a0f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4.1-nano-2025-04-14\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "62d88003",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are an expert Data Scientist. Your task is to generate a step-by-step list of Exploratory Data Analysis (EDA) tasks tailored to the given data type, with a focus on supporting downstream agents for feature engineering, model training, and evaluation.\n",
    "\n",
    "Requirements:\n",
    "- All tasks should be designed so that their outputs (important textual or numeric summaries, statistics, or lists) are logged into a structured JSON file, not displayed.\n",
    "- Tasks must begin with data loading and proceed through all essential EDA steps, including identifying data types, missing values, statistical summaries, cardinality, outlier detection, and any domain-specific EDA needed for modeling.\n",
    "- Avoid tasks that only generate visualizations unless the underlying data/summary is also saved as JSON.\n",
    "- Each task should be written as a single string, achievable via Python, and focus on producing outputs that can be consumed programmatically by downstream agents.\n",
    "- Do not output code, explanations, or any text outside the Python list of task descriptions.\n",
    "\n",
    "Input:\n",
    "- Data type: csv\n",
    "\n",
    "Output format:\n",
    "A Python list of EDA task descriptions as strings, with each task specifically designed so its results are logged into a JSON file for use by downstream agents. Make sure to save all the results in the JSON file and all values to be logged are JSON serializable.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4a42af76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='[\\n    \"Load the CSV data into a pandas DataFrame and save the shape (number of rows and columns) as a dictionary in the JSON file under the key \\'data_shape\\'.\",\\n    \"Identify data types for each column and save the data types as a dictionary in the JSON file under the key \\'column_types\\'.\",\\n    \"Calculate the total number of missing values per column and save as a dictionary in the JSON file under the key \\'missing_values\\'.\",\\n    \"Generate a list of unique values and their counts for each categorical column and save as a dictionary of dictionaries in the JSON file under the key \\'categorical_value_counts\\'.\",\\n    \"Compute basic statistical summaries (mean, median, min, max, standard deviation, quartiles) for all numerical columns and save as a dictionary in the JSON file under the key \\'numerical_summary\\'.\",\\n    \"Determine the number of unique values (cardinality) for each categorical and numerical column and save as a dictionary in the JSON file under the key \\'column_cardinality\\'.\",\\n    \"Identify potential outliers in numerical columns using the IQR method, and save the outlier counts per column as a dictionary in the JSON file under the key \\'numerical_outliers\\'.\",\\n    \"Calculate skewness and kurtosis for numerical columns and save these statistics as a dictionary in the JSON file under the key \\'distribution_stats\\'.\",\\n    \"Assess the correlation matrix for numerical features using Pearson correlation, and save the upper triangle of the correlation matrix as a nested dictionary in the JSON file under the key \\'correlation_matrix\\'.\",\\n    \"Identify columns with high missing value ratios (e.g., > 50%) and save their names and missing ratios as a list of dictionaries in the JSON file under the key \\'high_missing_columns\\'.\",\\n    \"For each categorical column, compute the mode (most frequent value) and its frequency, saving the results as a list of dictionaries in the JSON file under the key \\'categorical_modes\\'.\",\\n    \"Detect potential data quality issues such as duplicate rows, and save the count of duplicate rows in the JSON file under the key \\'duplicate_rows\\'.\",\\n    \"Summarize the distribution of target variable (if specified) including class counts or value counts, and save as a dictionary in the JSON file under the key \\'target_distribution\\'.\",\\n    \"Identify and log the presence of any constant or near-constant features (columns with very low variance) as a list in the JSON file under the key \\'constant_features\\'.\",\\n    \"Perform domain-specific checks if applicable (e.g., date ranges, known value constraints) and save relevant findings as a dictionary in the JSON file under the key \\'domain_checks\\'.\"\\n]', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 545, 'prompt_tokens': 282, 'total_tokens': 827, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-BiZokdZbaNq2NqtaCGLcVu7yuJQ1F', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--2e0c8eb6-b4ce-4760-98bd-43ed9a39c331-0', usage_metadata={'input_tokens': 282, 'output_tokens': 545, 'total_tokens': 827, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are an expert Prompt Enginner.\",\n",
    "    ),\n",
    "    (\"human\", prompt),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5d36ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = ai_msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5a86f7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    \"Load the CSV dataset into a pandas DataFrame and save its shape (number of rows and columns) into the JSON log under 'dataset_shape'.\",\n",
      "    \"Identify the data types for each column in the DataFrame and save the data types dictionary into the JSON log under 'column_data_types'.\",\n",
      "    \"Compute the number and percentage of missing values for each column and save this information into the JSON log under 'missing_value_counts_and_percentages'.\",\n",
      "    \"Calculate basic statistical summaries (mean, median, min, max, standard deviation, quartiles) for all numeric columns and save the summary statistics into the JSON log under 'numeric_summary_statistics'.\",\n",
      "    \"Calculate basic statistical summaries (count, unique values, top, frequency) for all categorical columns and save the categorical summaries into the JSON log under 'categorical_summary_statistics'.\",\n",
      "    \"Determine the cardinality (number of unique values) for each column and save these counts into the JSON log under 'column_cardinalities'.\",\n",
      "    \"Identify columns with high missing value ratios (above a specified threshold, e.g., 50%) and save the list of such columns along with their missing ratios into the JSON log under 'high_missing_value_columns'.\",\n",
      "    \"Detect potential outliers in numeric columns using the Interquartile Range (IQR) method, and save the outlier counts and indices for each numeric column into the JSON log under 'numeric_outliers'.\",\n",
      "    \"For each categorical column, compute the frequency distribution of each category and save the top 5 most frequent categories with their counts into the JSON log under 'categorical_top_categories'.\",\n",
      "    \"Identify columns with low variance (e.g., less than 1% variance) in their numeric values and save their names into the JSON log under 'low_variance_numeric_columns'.\",\n",
      "    \"Summarize the distribution of target variables if present, including class counts for classification tasks, paginated to support class imbalance detection, and save into the JSON log under 'target_variable_distribution'.\",\n",
      "    \"Perform correlation analysis among numeric features using Pearson correlation coefficients, and save the correlation matrix as a nested list into the JSON log under 'numeric_feature_correlation_matrix'.\",\n",
      "    \"Generate a list of feature columns that are highly correlated (above a threshold, e.g., 0.8 or below -0.8) with the target variable, saved under 'highly_correlated_features_with_target'.\",\n",
      "    \"Summarize and log any domain-specific data quality issues observed during EDA, such as duplicated entries, inconsistent categories, or suspicious patterns, under 'data_quality_notes'.\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1-nano-2025-04-14\",\n",
    "    input=prompt\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9b2a3098",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_text = response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8d4b4884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Certainly! Below is a comprehensive Python script that performs all the listed data analysis tasks on a CSV dataset, saving the results into a JSON log. You need to specify the CSV filename and optionally the target variable name (for classification tasks), as well as thresholds like missing value ratio, correlation cutoff, and variance cutoff.\\n\\n```python\\nimport pandas as pd\\nimport json\\nimport numpy as np\\n\\n# Configuration\\nCSV_FILE_PATH = \\'your_dataset.csv\\'  # Replace with your CSV file path\\nJSON_LOG_PATH = \\'data_analysis_log.json\\'\\nTARGET_VARIABLE = \\'target\\'  # Set to None if no target variable\\nMISSING_VALUE_THRESHOLD = 0.5  # 50%\\nCORRELATION_THRESHOLD = 0.8\\nLOW_VARIANCE_THRESHOLD = 0.01  # 1%\\n\\n# Load dataset\\ndf = pd.read_csv(CSV_FILE_PATH)\\n\\n# Initialize log dictionary\\nlog = {}\\n\\n# 1. Dataset shape\\nlog[\\'dataset_shape\\'] = df.shape\\n\\n# 2. Data types\\nlog[\\'column_data_types\\'] = df.dtypes.astype(str).to_dict()\\n\\n# 3. Missing values\\nmissing_counts = df.isnull().sum()\\nmissing_percentages = (missing_counts / len(df))\\nlog[\\'missing_value_counts_and_percentages\\'] = {\\n    col: {\\n        \\'missing_count\\': int(count),\\n        \\'missing_percentage\\': float(perc)\\n    }\\n    for col, count, perc in zip(df.columns, missing_counts, missing_percentages)\\n}\\n\\n# 4. Numeric summary statistics\\nnumeric_cols = df.select_dtypes(include=[np.number]).columns\\nnumeric_summary = df[numeric_cols].describe().T\\n# Add median separately\\nnumeric_summary[\\'median\\'] = df[numeric_cols].median()\\nlog[\\'numeric_summary_statistics\\'] = numeric_summary.to_dict()\\n\\n# 5. Categorical summary statistics\\ncategorical_cols = df.select_dtypes(include=[\\'object\\', \\'category\\']).columns\\ncategorical_summary = {}\\nfor col in categorical_cols:\\n    value_counts = df[col].value_counts(dropna=False)\\n    categorical_summary[col] = {\\n        \\'count\\': value_counts.to_dict(),\\n        \\'unique\\': df[col].nunique(),\\n        \\'top\\': value_counts.idxmax(),\\n        \\'top_freq\\': int(value_counts.max())\\n    }\\nlog[\\'categorical_summary_statistics\\'] = categorical_summary\\n\\n# 6. Column cardinalities\\ncolumn_cardinalities = df.nunique()\\nlog[\\'column_cardinalities\\'] = column_cardinalities.to_dict()\\n\\n# 7. Columns with high missing ratio\\nhigh_missing_cols = missing_percentages[missing_percentages > MISSING_VALUE_THRESHOLD]\\nlog[\\'high_missing_value_columns\\'] = {\\n    col: float(perc)\\n    for col, perc in high_missing_cols.items()\\n}\\n\\n# 8. Outliers in numeric columns (IQR method)\\nnumeric_outliers = {}\\nfor col in numeric_cols:\\n    Q1 = df[col].quantile(0.25)\\n    Q3 = df[col].quantile(0.75)\\n    IQR = Q3 - Q1\\n    lower_bound = Q1 - 1.5 * IQR\\n    upper_bound = Q3 + 1.5 * IQR\\n    outlier_mask = (df[col] < lower_bound) | (df[col] > upper_bound)\\n    outlier_indices = df.index[outlier_mask].tolist()\\n    numeric_outliers[col] = {\\n        \\'outlier_count\\': int(outlier_mask.sum()),\\n        \\'outlier_indices\\': outlier_indices\\n    }\\nlog[\\'numeric_outliers\\'] = numeric_outliers\\n\\n# 9. Top 5 categories per categorical column\\ncategorical_top_categories = {}\\nfor col in categorical_cols:\\n    value_counts = df[col].value_counts().head(5)\\n    categorical_top_categories[col] = value_counts.to_dict()\\nlog[\\'categorical_top_categories\\'] = categorical_top_categories\\n\\n# 10. Low variance numeric columns\\nlow_variance_cols = []\\nfor col in numeric_cols:\\n    if df[col].var() < LOW_VARIANCE_THRESHOLD:\\n        low_variance_cols.append(col)\\nlog[\\'low_variance_numeric_columns\\'] = low_variance_cols\\n\\n# 11. Target variable distribution (if specified)\\nif TARGET_VARIABLE and TARGET_VARIABLE in df.columns:\\n    target_counts = df[TARGET_VARIABLE].value_counts()\\n    # For classification, show class counts; for regression, maybe stats\\n    log[\\'target_variable_distribution\\'] = target_counts.to_dict()\\n\\n# 12. Correlation matrix of numeric features\\ncorrelation_matrix = df[numeric_cols].corr(method=\\'pearson\\')\\n# Convert to nested list\\nlog[\\'numeric_feature_correlation_matrix\\'] = correlation_matrix.values.tolist()\\n\\n# 13. Highly correlated features with target\\nhigh_corr_features = []\\nif TARGET_VARIABLE and TARGET_VARIABLE in df.columns:\\n    target_corrs = correlation_matrix[TARGET_VARIABLE]\\n    for col in numeric_cols:\\n        if col != TARGET_VARIABLE:\\n            corr_value = target_corrs[col]\\n            if abs(corr_value) > CORRELATION_THRESHOLD:\\n                high_corr_features.append(col)\\nlog[\\'highly_correlated_features_with_target\\'] = high_corr_features\\n\\n# 14. Domain-specific data quality notes (manual or heuristic)\\n# For illustration, let\\'s check for duplicated entries and suspicious patterns\\nduplicates = df.duplicated()\\ndata_quality_notes = []\\nif duplicates.any():\\n    data_quality_notes.append(f\"Found {duplicates.sum()} duplicated rows.\")\\n# Check for inconsistent categories (e.g., presence of unexpected categories)\\n# Example (can be customized)\\n# For simplicity, assume no specific rules; expand as needed\\nlog[\\'data_quality_notes\\'] = data_quality_notes\\n\\n# Save the log to JSON\\nwith open(JSON_LOG_PATH, \\'w\\') as f:\\n    json.dump(log, f, indent=4)\\n\\nprint(f\"Analysis complete. Log saved to {JSON_LOG_PATH}.\")\\n```\\n\\n### How to use:\\n\\n1. Replace `\\'your_dataset.csv\\'` with the actual path to your dataset.\\n2. Set `TARGET_VARIABLE` if your dataset has a target column.\\n3. Adjust thresholds if needed.\\n4. Run the script.\\n\\nThis will generate a detailed JSON report covering all the requested analyses.\\n\\nLet me know if you\\'d like a specific part expanded or customized!'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "        model=\"gpt-4.1-nano-2025-04-14\",\n",
    "        input= f\"write python code for {list_text}\"\n",
    "    )\n",
    "response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d2756499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_python_code(text):\n",
    "    \"\"\"\n",
    "    Extracts Python code blocks enclosed in ```python ... ``` from a given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing the code.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of extracted Python code strings.\n",
    "    \"\"\"\n",
    "    # The regex looks for:\n",
    "    # ```python          - literal start of the code block marker\n",
    "    # \\s* - optional whitespace (e.g., newline)\n",
    "    # (.*?)              - non-greedy capture of any character (the code itself)\n",
    "    # \\s* - optional whitespace before the closing marker\n",
    "    # ```                - literal end of the code block marker\n",
    "    python_code_blocks = re.findall(r\"```python\\s*(.*?)\\s*```\", text, re.DOTALL)\n",
    "    return [block.strip() for block in python_code_blocks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9698cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_code = extract_python_code(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4764049a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nimport json\\nimport numpy as np\\n\\n# Configuration\\nCSV_FILE_PATH = \\'your_dataset.csv\\'  # Replace with your CSV file path\\nJSON_LOG_PATH = \\'data_analysis_log.json\\'\\nTARGET_VARIABLE = \\'target\\'  # Set to None if no target variable\\nMISSING_VALUE_THRESHOLD = 0.5  # 50%\\nCORRELATION_THRESHOLD = 0.8\\nLOW_VARIANCE_THRESHOLD = 0.01  # 1%\\n\\n# Load dataset\\ndf = pd.read_csv(CSV_FILE_PATH)\\n\\n# Initialize log dictionary\\nlog = {}\\n\\n# 1. Dataset shape\\nlog[\\'dataset_shape\\'] = df.shape\\n\\n# 2. Data types\\nlog[\\'column_data_types\\'] = df.dtypes.astype(str).to_dict()\\n\\n# 3. Missing values\\nmissing_counts = df.isnull().sum()\\nmissing_percentages = (missing_counts / len(df))\\nlog[\\'missing_value_counts_and_percentages\\'] = {\\n    col: {\\n        \\'missing_count\\': int(count),\\n        \\'missing_percentage\\': float(perc)\\n    }\\n    for col, count, perc in zip(df.columns, missing_counts, missing_percentages)\\n}\\n\\n# 4. Numeric summary statistics\\nnumeric_cols = df.select_dtypes(include=[np.number]).columns\\nnumeric_summary = df[numeric_cols].describe().T\\n# Add median separately\\nnumeric_summary[\\'median\\'] = df[numeric_cols].median()\\nlog[\\'numeric_summary_statistics\\'] = numeric_summary.to_dict()\\n\\n# 5. Categorical summary statistics\\ncategorical_cols = df.select_dtypes(include=[\\'object\\', \\'category\\']).columns\\ncategorical_summary = {}\\nfor col in categorical_cols:\\n    value_counts = df[col].value_counts(dropna=False)\\n    categorical_summary[col] = {\\n        \\'count\\': value_counts.to_dict(),\\n        \\'unique\\': df[col].nunique(),\\n        \\'top\\': value_counts.idxmax(),\\n        \\'top_freq\\': int(value_counts.max())\\n    }\\nlog[\\'categorical_summary_statistics\\'] = categorical_summary\\n\\n# 6. Column cardinalities\\ncolumn_cardinalities = df.nunique()\\nlog[\\'column_cardinalities\\'] = column_cardinalities.to_dict()\\n\\n# 7. Columns with high missing ratio\\nhigh_missing_cols = missing_percentages[missing_percentages > MISSING_VALUE_THRESHOLD]\\nlog[\\'high_missing_value_columns\\'] = {\\n    col: float(perc)\\n    for col, perc in high_missing_cols.items()\\n}\\n\\n# 8. Outliers in numeric columns (IQR method)\\nnumeric_outliers = {}\\nfor col in numeric_cols:\\n    Q1 = df[col].quantile(0.25)\\n    Q3 = df[col].quantile(0.75)\\n    IQR = Q3 - Q1\\n    lower_bound = Q1 - 1.5 * IQR\\n    upper_bound = Q3 + 1.5 * IQR\\n    outlier_mask = (df[col] < lower_bound) | (df[col] > upper_bound)\\n    outlier_indices = df.index[outlier_mask].tolist()\\n    numeric_outliers[col] = {\\n        \\'outlier_count\\': int(outlier_mask.sum()),\\n        \\'outlier_indices\\': outlier_indices\\n    }\\nlog[\\'numeric_outliers\\'] = numeric_outliers\\n\\n# 9. Top 5 categories per categorical column\\ncategorical_top_categories = {}\\nfor col in categorical_cols:\\n    value_counts = df[col].value_counts().head(5)\\n    categorical_top_categories[col] = value_counts.to_dict()\\nlog[\\'categorical_top_categories\\'] = categorical_top_categories\\n\\n# 10. Low variance numeric columns\\nlow_variance_cols = []\\nfor col in numeric_cols:\\n    if df[col].var() < LOW_VARIANCE_THRESHOLD:\\n        low_variance_cols.append(col)\\nlog[\\'low_variance_numeric_columns\\'] = low_variance_cols\\n\\n# 11. Target variable distribution (if specified)\\nif TARGET_VARIABLE and TARGET_VARIABLE in df.columns:\\n    target_counts = df[TARGET_VARIABLE].value_counts()\\n    # For classification, show class counts; for regression, maybe stats\\n    log[\\'target_variable_distribution\\'] = target_counts.to_dict()\\n\\n# 12. Correlation matrix of numeric features\\ncorrelation_matrix = df[numeric_cols].corr(method=\\'pearson\\')\\n# Convert to nested list\\nlog[\\'numeric_feature_correlation_matrix\\'] = correlation_matrix.values.tolist()\\n\\n# 13. Highly correlated features with target\\nhigh_corr_features = []\\nif TARGET_VARIABLE and TARGET_VARIABLE in df.columns:\\n    target_corrs = correlation_matrix[TARGET_VARIABLE]\\n    for col in numeric_cols:\\n        if col != TARGET_VARIABLE:\\n            corr_value = target_corrs[col]\\n            if abs(corr_value) > CORRELATION_THRESHOLD:\\n                high_corr_features.append(col)\\nlog[\\'highly_correlated_features_with_target\\'] = high_corr_features\\n\\n# 14. Domain-specific data quality notes (manual or heuristic)\\n# For illustration, let\\'s check for duplicated entries and suspicious patterns\\nduplicates = df.duplicated()\\ndata_quality_notes = []\\nif duplicates.any():\\n    data_quality_notes.append(f\"Found {duplicates.sum()} duplicated rows.\")\\n# Check for inconsistent categories (e.g., presence of unexpected categories)\\n# Example (can be customized)\\n# For simplicity, assume no specific rules; expand as needed\\nlog[\\'data_quality_notes\\'] = data_quality_notes\\n\\n# Save the log to JSON\\nwith open(JSON_LOG_PATH, \\'w\\') as f:\\n    json.dump(log, f, indent=4)\\n\\nprint(f\"Analysis complete. Log saved to {JSON_LOG_PATH}.\")'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_code[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "578071d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code successfully saved to 'my_script-gpt.py'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Define the filename for the Python script\n",
    "filename = \"my_script-gpt.py\"\n",
    "\n",
    "try:\n",
    "    # Save the code to a file\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(extracted_code[0])\n",
    "    print(f\"Code successfully saved to '{filename}'\")\n",
    "\n",
    "except IOError as e:\n",
    "    print(f\"An error occurred while writing to the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c2ffd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "742e7111",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8774a12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# --- Configuration ---\n",
    "# The path to your JSON file containing the data description.\n",
    "# Make sure this file exists in the same directory, or provide a full path.\n",
    "json_file_path = 'data_analysis_log.json'\n",
    "\n",
    "prompt_template = f\"\"\"\n",
    "You are an expert Machine Learning Engineer. Your task is to generate a step-by-step list of tasks for training and evaluating a machine learning model using structured tabular data.\n",
    "\n",
    "Context:\n",
    "- The input data is in CSV format.\n",
    "- A previous agent (EDA agent) has already analyzed the dataset and generated a structured JSON file named {json_file_path}. This file contains outputs such as data types, missing value statistics, cardinality, outlier info, and distribution summaries.\n",
    "\n",
    "Requirements:\n",
    "- All tasks should use insights from the EDA JSON file ({json_file_path}) where appropriate.\n",
    "- Each task should produce outputs (such as selected features, cleaned dataset paths, model hyperparameters, evaluation metrics, or model file paths) that are JSON-serializable and must be logged into a structured JSON file. This JSON will be consumed by downstream agents for deployment, explanation, or monitoring.\n",
    "- Tasks must begin with reading the EDA output and continue through preprocessing, feature selection, train-test splitting, model training, evaluation, and saving of final artifacts.\n",
    "- Focus on general-purpose supervised machine learning (classification or regression depending on the target variable).\n",
    "- Avoid any tasks that only generate visualizations unless their summaries or values are saved in structured form.\n",
    "- Each task must be expressed as a single string that could be executed in Python and designed to run sequentially.\n",
    "- Do not output code, explanations, or any text outside the Python list of task descriptions.\n",
    "\n",
    "Input:\n",
    "- Data type: csv\n",
    "- EDA results: structured JSON file \n",
    "- dataset path: 'iris.csv'\n",
    "\n",
    "Output format:\n",
    "A Python list of model training task descriptions as strings, with each task specifically designed so its results are logged into a structured JSON file for use by downstream agents.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "fa4cb9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    \"Load the data_analysis_log.json file and parse it to extract insights about data types, missing values, categorical features, and distribution summaries.\",\n",
      "    \"Read the iris.csv dataset into a pandas DataFrame and save it to a temporary file path 'data_raw.csv'.\",\n",
      "    \"Identify features with missing values from the EDA log; for features with missing data, perform imputation using the median for numerical features and mode for categorical features, then save the cleaned dataset to 'data_imputed.csv'.\",\n",
      "    \"Using the data types and outlier info from the EDA log, normalize numerical features by applying MinMaxScaler, and encode categorical features with OneHotEncoder, saving the processed dataset to 'data_processed.csv'.\",\n",
      "    \"Analyze the distribution summaries and cardinality info from the EDA log to select core features; save the list of selected feature names in a JSON file as 'selected_features.json'.\",\n",
      "    \"Split the processed dataset into features (X) and target variable (y) based on EDA insights, then perform a train-test split with stratification if classification, and save train and test datasets to 'train_data.csv' and 'test_data.csv'.\",\n",
      "    \"Identify the target variable's type (classification or regression) from the EDA log; if classification, encode the target variable using LabelEncoder; if regression, leave as is, and save the encoded target to 'y_train.json' and 'y_test.json'.\",\n",
      "    \"Configure a machine learning model pipeline (e.g., RandomForestClassifier for classification or RandomForestRegressor for regression) with initial hyperparameters; save the hyperparameters into 'model_params.json'.\",\n",
      "    \"Train the model pipeline using the training data, then save the trained model to a file named 'final_model.pkl'.\",\n",
      "    \"Evaluate the trained model on the test dataset by calculating appropriate metrics: accuracy, precision, recall, and F1-score for classification or RMSE and R^2 for regression; save the evaluation report as 'evaluation_metrics.json'.\",\n",
      "    \"Log the paths to the cleaned data, feature list, model parameters, trained model, and evaluation metrics into a master JSON file 'training_log.json' for downstream consumption.\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1-nano-2025-04-14\",\n",
    "    input=prompt_template\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a741ac41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Certainly! Here\\'s a comprehensive Python script that executes all the steps you\\'ve outlined, along with explanations. Note that I\\'ll include some mock assumptions where specifics are missing (such as the structure of the `data_analysis_log.json`), and I\\'ll use common libraries like pandas, scikit-learn, and JSON.\\n\\n```python\\nimport json\\nimport pandas as pd\\nimport numpy as np\\nimport tempfile\\nimport os\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, r2_score\\nimport joblib\\n\\n# Load the data_analysis_log.json file and parse insights\\nwith open(\\'data_analysis_log.json\\', \\'r\\') as f:\\n    eda_log = json.load(f)\\n\\n# Assume eda_log is a dictionary with keys like:\\n# \\'data_types\\', \\'missing_values\\', \\'categorical_features\\', \\'distribution_summaries\\', \\'outlier_info\\', \\'core_features\\', \\'target_variable\\', \\'task_type\\'\\n\\n# Step 2: Read irish.csv into DataFrame\\ndf = pd.read_csv(\\'iris.csv\\')  # assuming \\'iris.csv\\' exists\\ntemp_dir = tempfile.mkdtemp()\\nraw_csv_path = os.path.join(temp_dir, \\'data_raw.csv\\')\\ndf.to_csv(raw_csv_path, index=False)\\n\\n# Step 3: Handle missing values\\n# Identify features with missingness from the EDA log\\nmissing_info = eda_log.get(\\'missing_values\\', {})\\nfeatures_with_missing = [feat for feat, val in missing_info.items() if val > 0]\\n\\n# Determine feature types\\ndata_types = eda_log.get(\\'data_types\\', {})  # {\\'feat1\\': \\'numeric\\', \\'feat2\\': \\'categorical\\', ...}\\n\\n# Prepare imputer strategies\\nfor feature in features_with_missing:\\n    if data_types.get(feature) == \\'numeric\\':\\n        median_val = df[feature].median()\\n        df[feature].fillna(median_val, inplace=True)\\n    else:\\n        mode_val = df[feature].mode()[0]\\n        df[feature].fillna(mode_val, inplace=True)\\n\\n# Save cleaned data\\nimputed_csv_path = os.path.join(temp_dir, \\'data_imputed.csv\\')\\ndf.to_csv(imputed_csv_path, index=False)\\n\\n# Step 4: Normalize numerical features and encode categorical features\\nnumerical_features = [feat for feat, dtype in data_types.items() if dtype == \\'numeric\\']\\ncategorical_features = [feat for feat, dtype in data_types.items() if dtype == \\'categorical\\']\\n\\n# Define transformers\\npreprocessor = ColumnTransformer(\\n    transformers=[\\n        (\\'num\\', MinMaxScaler(), numerical_features),\\n        (\\'cat\\', OneHotEncoder(handle_unknown=\\'ignore\\'), categorical_features)\\n    ]\\n)\\n\\n# Fit and transform data\\nX = df.drop(columns=[\\'target\\'])  # assuming \\'target\\' is the target; adjust as needed\\ny = df[\\'target\\']\\n\\nX_processed = preprocessor.fit_transform(X)\\n\\n# Save processed data\\nprocessed_df = pd.DataFrame(X_processed, columns=preprocessor.get_feature_names_out())\\nprocessed_df[\\'target\\'] = y.values\\nprocessed_csv_path = os.path.join(temp_dir, \\'data_processed.csv\\')\\nprocessed_df.to_csv(processed_csv_path, index=False)\\n\\n# Step 5: Select core features based on distribution and cardinality info\\ndistribution_summaries = eda_log.get(\\'distribution_summaries\\', {})\\ncardinality_info = eda_log.get(\\'cardinality\\', {})  # e.g., {\\'feat1\\': 10, ...}\\ncore_features = eda_log.get(\\'core_features\\', [])  # assuming list provided\\n\\n# Save core features to JSON\\nselected_features_path = \\'selected_features.json\\'\\nwith open(selected_features_path, \\'w\\') as f:\\n    json.dump(core_features, f)\\n\\n# Step 6: Split data into train/test\\nX_core = df[core_features]\\ny_core = df[\\'target\\']\\n\\n# Determine if it\\'s classification or regression from EDA log\\ntask_type = eda_log.get(\\'task_type\\', \\'classification\\')  # default to classification\\n\\nif task_type == \\'classification\\':\\n    stratify = y_core\\nelse:\\n    stratify = None\\n\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X_core, y_core, test_size=0.2, stratify=stratify\\n)\\n\\n# Save train/test data\\nX_train.to_csv(\\'train_data.csv\\', index=False)\\nX_test.to_csv(\\'test_data.csv\\', index=False)\\n\\n# Step 7: Encode target variable if classification\\nif task_type == \\'classification\\':\\n    label_enc = LabelEncoder()\\n    y_train_enc = label_enc.fit_transform(y_train)\\n    y_test_enc = label_enc.transform(y_test)\\n    # Save encoded y\\n    with open(\\'y_train.json\\', \\'w\\') as f:\\n        json.dump({\\'y_train\\': y_train_enc.tolist()}, f)\\n    with open(\\'y_test.json\\', \\'w\\') as f:\\n        json.dump({\\'y_test\\': y_test_enc.tolist()}, f)\\n    y_train, y_test = y_train_enc, y_test_enc\\nelse:\\n    # For regression, save as is\\n    pd.DataFrame({\\'y_train\\': y_train}).to_json(\\'y_train.json\\')\\n    pd.DataFrame({\\'y_test\\': y_test}).to_json(\\'y_test.json\\')\\n\\n# Step 8: Configure model pipeline with hyperparameters\\nhyperparams = {\\n    \\'n_estimators\\': 100,\\n    \\'max_depth\\': None,\\n    \\'random_state\\': 42\\n}\\n# Save hyperparameters\\nwith open(\\'model_params.json\\', \\'w\\') as f:\\n    json.dump(hyperparams, f)\\n\\n# Define model\\nif task_type == \\'classification\\':\\n    model = RandomForestClassifier(**hyperparams)\\nelse:\\n    model = RandomForestRegressor(**hyperparams)\\n\\n# Save model parameters\\n# (Already saved as \\'model_params.json\\')\\n\\n# Step 9: Train the model\\nmodel.fit(X_train, y_train)\\n# Save trained model\\njoblib.dump(model, \\'final_model.pkl\\')\\n\\n# Step 10: Evaluate the model\\ny_pred = model.predict(X_test)\\n\\nif task_type == \\'classification\\':\\n    eval_metrics = {\\n        \\'accuracy\\': accuracy_score(y_test, y_pred),\\n        \\'precision\\': precision_score(y_test, y_pred, average=\\'weighted\\', zero_division=0),\\n        \\'recall\\': recall_score(y_test, y_pred, average=\\'weighted\\', zero_division=0),\\n        \\'f1_score\\': f1_score(y_test, y_pred, average=\\'weighted\\', zero_division=0)\\n    }\\nelse:\\n    eval_metrics = {\\n        \\'RMSE\\': mean_squared_error(y_test, y_pred, squared=False),\\n        \\'R2\\': r2_score(y_test, y_pred)\\n    }\\n\\n# Save evaluation metrics\\nwith open(\\'evaluation_metrics.json\\', \\'w\\') as f:\\n    json.dump(eval_metrics, f)\\n\\n# Step 11: Log paths to master JSON\\ntraining_log = {\\n    \\'cleaned_data_path\\': imputed_csv_path,\\n    \\'processed_data_path\\': processed_csv_path,\\n    \\'feature_list\\': selected_features_path,\\n    \\'model_params\\': \\'model_params.json\\',\\n    \\'trained_model\\': \\'final_model.pkl\\',\\n    \\'evaluation_metrics\\': \\'evaluation_metrics.json\\',\\n    \\'train_data_path\\': \\'train_data.csv\\',\\n    \\'test_data_path\\': \\'test_data.csv\\'\\n}\\n\\nwith open(\\'training_log.json\\', \\'w\\') as f:\\n    json.dump(training_log, f)\\n\\nprint(\"All steps completed successfully.\")\\n```\\n\\n### Notes:\\n- Make sure `data_analysis_log.json` is properly formatted and exists.\\n- Adjust column names such as `\\'target\\'` and feature lists based on your actual data.\\n- Install required packages:\\n\\n```bash\\npip install pandas scikit-learn joblib\\n```\\n\\n- This script creates temporary directories/files as needed and saves outputs to current directory or specified paths.\\n\\nLet me know if you\\'d like any part clarified or customized further!'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_text = response.output_text\n",
    "response = client.responses.create(\n",
    "        model=\"gpt-4.1-nano-2025-04-14\",\n",
    "        input= f\"write python code for {list_text}\"\n",
    "    )\n",
    "response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "bd798f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import json\\nimport pandas as pd\\nimport numpy as np\\nimport tempfile\\nimport os\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, r2_score\\nimport joblib\\n\\n# Load the data_analysis_log.json file and parse insights\\nwith open(\\'data_analysis_log.json\\', \\'r\\') as f:\\n    eda_log = json.load(f)\\n\\n# Assume eda_log is a dictionary with keys like:\\n# \\'data_types\\', \\'missing_values\\', \\'categorical_features\\', \\'distribution_summaries\\', \\'outlier_info\\', \\'core_features\\', \\'target_variable\\', \\'task_type\\'\\n\\n# Step 2: Read irish.csv into DataFrame\\ndf = pd.read_csv(\\'iris.csv\\')  # assuming \\'iris.csv\\' exists\\ntemp_dir = tempfile.mkdtemp()\\nraw_csv_path = os.path.join(temp_dir, \\'data_raw.csv\\')\\ndf.to_csv(raw_csv_path, index=False)\\n\\n# Step 3: Handle missing values\\n# Identify features with missingness from the EDA log\\nmissing_info = eda_log.get(\\'missing_values\\', {})\\nfeatures_with_missing = [feat for feat, val in missing_info.items() if val > 0]\\n\\n# Determine feature types\\ndata_types = eda_log.get(\\'data_types\\', {})  # {\\'feat1\\': \\'numeric\\', \\'feat2\\': \\'categorical\\', ...}\\n\\n# Prepare imputer strategies\\nfor feature in features_with_missing:\\n    if data_types.get(feature) == \\'numeric\\':\\n        median_val = df[feature].median()\\n        df[feature].fillna(median_val, inplace=True)\\n    else:\\n        mode_val = df[feature].mode()[0]\\n        df[feature].fillna(mode_val, inplace=True)\\n\\n# Save cleaned data\\nimputed_csv_path = os.path.join(temp_dir, \\'data_imputed.csv\\')\\ndf.to_csv(imputed_csv_path, index=False)\\n\\n# Step 4: Normalize numerical features and encode categorical features\\nnumerical_features = [feat for feat, dtype in data_types.items() if dtype == \\'numeric\\']\\ncategorical_features = [feat for feat, dtype in data_types.items() if dtype == \\'categorical\\']\\n\\n# Define transformers\\npreprocessor = ColumnTransformer(\\n    transformers=[\\n        (\\'num\\', MinMaxScaler(), numerical_features),\\n        (\\'cat\\', OneHotEncoder(handle_unknown=\\'ignore\\'), categorical_features)\\n    ]\\n)\\n\\n# Fit and transform data\\nX = df.drop(columns=[\\'target\\'])  # assuming \\'target\\' is the target; adjust as needed\\ny = df[\\'target\\']\\n\\nX_processed = preprocessor.fit_transform(X)\\n\\n# Save processed data\\nprocessed_df = pd.DataFrame(X_processed, columns=preprocessor.get_feature_names_out())\\nprocessed_df[\\'target\\'] = y.values\\nprocessed_csv_path = os.path.join(temp_dir, \\'data_processed.csv\\')\\nprocessed_df.to_csv(processed_csv_path, index=False)\\n\\n# Step 5: Select core features based on distribution and cardinality info\\ndistribution_summaries = eda_log.get(\\'distribution_summaries\\', {})\\ncardinality_info = eda_log.get(\\'cardinality\\', {})  # e.g., {\\'feat1\\': 10, ...}\\ncore_features = eda_log.get(\\'core_features\\', [])  # assuming list provided\\n\\n# Save core features to JSON\\nselected_features_path = \\'selected_features.json\\'\\nwith open(selected_features_path, \\'w\\') as f:\\n    json.dump(core_features, f)\\n\\n# Step 6: Split data into train/test\\nX_core = df[core_features]\\ny_core = df[\\'target\\']\\n\\n# Determine if it\\'s classification or regression from EDA log\\ntask_type = eda_log.get(\\'task_type\\', \\'classification\\')  # default to classification\\n\\nif task_type == \\'classification\\':\\n    stratify = y_core\\nelse:\\n    stratify = None\\n\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X_core, y_core, test_size=0.2, stratify=stratify\\n)\\n\\n# Save train/test data\\nX_train.to_csv(\\'train_data.csv\\', index=False)\\nX_test.to_csv(\\'test_data.csv\\', index=False)\\n\\n# Step 7: Encode target variable if classification\\nif task_type == \\'classification\\':\\n    label_enc = LabelEncoder()\\n    y_train_enc = label_enc.fit_transform(y_train)\\n    y_test_enc = label_enc.transform(y_test)\\n    # Save encoded y\\n    with open(\\'y_train.json\\', \\'w\\') as f:\\n        json.dump({\\'y_train\\': y_train_enc.tolist()}, f)\\n    with open(\\'y_test.json\\', \\'w\\') as f:\\n        json.dump({\\'y_test\\': y_test_enc.tolist()}, f)\\n    y_train, y_test = y_train_enc, y_test_enc\\nelse:\\n    # For regression, save as is\\n    pd.DataFrame({\\'y_train\\': y_train}).to_json(\\'y_train.json\\')\\n    pd.DataFrame({\\'y_test\\': y_test}).to_json(\\'y_test.json\\')\\n\\n# Step 8: Configure model pipeline with hyperparameters\\nhyperparams = {\\n    \\'n_estimators\\': 100,\\n    \\'max_depth\\': None,\\n    \\'random_state\\': 42\\n}\\n# Save hyperparameters\\nwith open(\\'model_params.json\\', \\'w\\') as f:\\n    json.dump(hyperparams, f)\\n\\n# Define model\\nif task_type == \\'classification\\':\\n    model = RandomForestClassifier(**hyperparams)\\nelse:\\n    model = RandomForestRegressor(**hyperparams)\\n\\n# Save model parameters\\n# (Already saved as \\'model_params.json\\')\\n\\n# Step 9: Train the model\\nmodel.fit(X_train, y_train)\\n# Save trained model\\njoblib.dump(model, \\'final_model.pkl\\')\\n\\n# Step 10: Evaluate the model\\ny_pred = model.predict(X_test)\\n\\nif task_type == \\'classification\\':\\n    eval_metrics = {\\n        \\'accuracy\\': accuracy_score(y_test, y_pred),\\n        \\'precision\\': precision_score(y_test, y_pred, average=\\'weighted\\', zero_division=0),\\n        \\'recall\\': recall_score(y_test, y_pred, average=\\'weighted\\', zero_division=0),\\n        \\'f1_score\\': f1_score(y_test, y_pred, average=\\'weighted\\', zero_division=0)\\n    }\\nelse:\\n    eval_metrics = {\\n        \\'RMSE\\': mean_squared_error(y_test, y_pred, squared=False),\\n        \\'R2\\': r2_score(y_test, y_pred)\\n    }\\n\\n# Save evaluation metrics\\nwith open(\\'evaluation_metrics.json\\', \\'w\\') as f:\\n    json.dump(eval_metrics, f)\\n\\n# Step 11: Log paths to master JSON\\ntraining_log = {\\n    \\'cleaned_data_path\\': imputed_csv_path,\\n    \\'processed_data_path\\': processed_csv_path,\\n    \\'feature_list\\': selected_features_path,\\n    \\'model_params\\': \\'model_params.json\\',\\n    \\'trained_model\\': \\'final_model.pkl\\',\\n    \\'evaluation_metrics\\': \\'evaluation_metrics.json\\',\\n    \\'train_data_path\\': \\'train_data.csv\\',\\n    \\'test_data_path\\': \\'test_data.csv\\'\\n}\\n\\nwith open(\\'training_log.json\\', \\'w\\') as f:\\n    json.dump(training_log, f)\\n\\nprint(\"All steps completed successfully.\")'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_code = extract_python_code(response.output_text)\n",
    "extracted_code[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6ee692d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code successfully saved to 'model_training_gpt.py'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Define the filename for the Python script\n",
    "filename = \"model_training_gpt.py\"\n",
    "\n",
    "try:\n",
    "    # Save the code to a file\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(extracted_code[0])\n",
    "    print(f\"Code successfully saved to '{filename}'\")\n",
    "\n",
    "except IOError as e:\n",
    "    print(f\"An error occurred while writing to the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4b7854f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ExecutionResult:\n",
    "    success: bool\n",
    "    stdout: str\n",
    "    stderr: str\n",
    "\n",
    "class PythonCodeExecutor:\n",
    "    def __init__(self, timeout: int = 10):\n",
    "        self.timeout = timeout\n",
    "\n",
    "    def execute(self, code: str) -> ExecutionResult:\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as temp_file:\n",
    "            temp_file.write(code)\n",
    "            temp_file_path = temp_file.name\n",
    "\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                ['python', temp_file_path],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            return ExecutionResult(\n",
    "                success=result.returncode == 0,\n",
    "                stdout=result.stdout,\n",
    "                stderr=result.stderr\n",
    "            )\n",
    "        except subprocess.TimeoutExpired as e:\n",
    "            return ExecutionResult(success=False, stdout='', stderr='Execution timed out.')\n",
    "        except Exception as e:\n",
    "            return ExecutionResult(success=False, stdout='', stderr=str(e))\n",
    "        finally:\n",
    "            os.remove(temp_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "382475f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: \n",
      "STDERR: Traceback (most recent call last):\n",
      "  File \u001b[35m\"/var/folders/s1/s3f_rpyj6zv3xss6397vnq0m0000gn/T/tmpc941fqqg.py\"\u001b[0m, line \u001b[35m62\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    X = df.drop(columns=['target'])  # assuming 'target' is the target; adjust as needed\n",
      "  File \u001b[35m\"/opt/anaconda3/envs/agentml/lib/python3.13/site-packages/pandas/core/frame.py\"\u001b[0m, line \u001b[35m5588\u001b[0m, in \u001b[35mdrop\u001b[0m\n",
      "    return \u001b[31msuper().drop\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mlabels=labels,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
      "    ...<5 lines>...\n",
      "        \u001b[1;31merrors=errors,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/opt/anaconda3/envs/agentml/lib/python3.13/site-packages/pandas/core/generic.py\"\u001b[0m, line \u001b[35m4807\u001b[0m, in \u001b[35mdrop\u001b[0m\n",
      "    obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n",
      "  File \u001b[35m\"/opt/anaconda3/envs/agentml/lib/python3.13/site-packages/pandas/core/generic.py\"\u001b[0m, line \u001b[35m4849\u001b[0m, in \u001b[35m_drop_axis\u001b[0m\n",
      "    new_axis = axis.drop(labels, errors=errors)\n",
      "  File \u001b[35m\"/opt/anaconda3/envs/agentml/lib/python3.13/site-packages/pandas/core/indexes/base.py\"\u001b[0m, line \u001b[35m7098\u001b[0m, in \u001b[35mdrop\u001b[0m\n",
      "    raise KeyError(f\"{labels[mask].tolist()} not found in axis\")\n",
      "\u001b[1;35mKeyError\u001b[0m: \u001b[35m\"['target'] not found in axis\"\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    executor = PythonCodeExecutor()\n",
    "    code = extracted_code[0]\n",
    "    result = executor.execute(code)\n",
    "    print(\"STDOUT:\", result.stdout)\n",
    "    print(\"STDERR:\", result.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290d457c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
